{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim verÃ¤ndern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from bfh_mt_hs2020_rl_basics.agent import AgentBase\n",
    "\n",
    "from typing import Iterable, Tuple, List\n",
    "import numpy as np\n",
    "\n",
    "from ignite.engine import Engine\n",
    "\n",
    "from ptan.experience import ExperienceFirstLast\n",
    "\n",
    "import torch\n",
    "from torch.optim import Optimizer, Adam\n",
    "from torch import device\n",
    "\n",
    "class BridgeBase(ABC):\n",
    "    \n",
    "    def __init__(self, agent: AgentBase, optimizer: Optimizer = None, \n",
    "                 learning_rate: float = 0.0001, \n",
    "                 gamma: float = 0.9, \n",
    "                 initial_population: int = 1000, \n",
    "                 batch_size: int = 32):\n",
    "        self.agent = agent\n",
    "        self.device = agent.device    \n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.initial_population = initial_population\n",
    "        self.batch_size = batch_size\n",
    "                \n",
    "        if optimizer is not None:\n",
    "            self.optimzer = optimizer\n",
    "        else:\n",
    "            self.optimizer = Adam(self.agent.net.parameters(), lr=learning_rate)\n",
    "    \n",
    "\n",
    "    def batch_generator(self):\n",
    "        self.agent.buffer.populate(self.initial_population)\n",
    "        while True:\n",
    "            self.agent.buffer.populate(1)\n",
    "            yield self.get_sample()\n",
    "\n",
    "\n",
    "    def _unpack_batch(self, batch: List[ExperienceFirstLast]):\n",
    "        states, actions, rewards, dones, last_states = [],[],[],[],[]\n",
    "        \n",
    "        for exp in batch:\n",
    "            state = np.array(exp.state)\n",
    "            states.append(state)\n",
    "            actions.append(exp.action)\n",
    "            rewards.append(exp.reward)\n",
    "            dones.append(exp.last_state is None)\n",
    "            \n",
    "            if exp.last_state is None:\n",
    "                lstate = state  # the result will be masked anyway\n",
    "            else:\n",
    "                lstate = np.array(exp.last_state)\n",
    "            last_states.append(lstate)\n",
    "            \n",
    "        return np.array(states, copy=False), \\\n",
    "               np.array(actions), \\\n",
    "               np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones,   dtype=np.uint8), \\\n",
    "               np.array(last_states, copy=False)     \n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_sample(self, engine: Engine, batchdata):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def process_batch(self, engine: Engine, batchdata):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleBridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from bfh_mt_hs2020_rl_basics.agent import AgentBase, SimpleAgent\n",
    "\n",
    "from typing import Iterable, Tuple, List\n",
    "import numpy as np\n",
    "\n",
    "from ignite.engine import Engine\n",
    "\n",
    "from ptan.experience import ExperienceFirstLast\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer, Adam\n",
    "from torch import device\n",
    "\n",
    "\n",
    "class SimpleBridge(BridgeBase):\n",
    "    \n",
    "    def __init__(self, agent: SimpleAgent, optimizer: Optimizer = None, \n",
    "                 learning_rate: float = 0.0001, \n",
    "                 gamma: float = 0.9, \n",
    "                 initial_population: int = 1000, \n",
    "                 batch_size: int = 32):\n",
    "        \n",
    "        super(SimpleBridge, self).__init__(agent, optimizer, learning_rate, gamma, initial_population, batch_size)\n",
    " \n",
    "\n",
    "    def get_sample(self):\n",
    "        return self.agent.buffer.sample(self.batch_size)\n",
    "\n",
    "    \n",
    "    def process_batch(self, engine:Engine, batchdata):\n",
    "        self.optimizer.zero_grad()\n",
    "       \n",
    "        loss_v = self._calc_loss(batchdata)\n",
    "\n",
    "        loss_v.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.agent.iteration_completed(engine.state.iteration)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss_v.item(),\n",
    "            \"epsilon\": self.agent.selector.epsilon,\n",
    "        }\n",
    "\n",
    "\n",
    "    def _calc_loss(self, batch: List[ExperienceFirstLast]):\n",
    "        \n",
    "        states, actions, rewards, dones, next_states = self._unpack_batch(batch)\n",
    "\n",
    "        states_v      = torch.tensor(states).to(self.device)\n",
    "        next_states_v = torch.tensor(next_states).to(self.device)\n",
    "        actions_v     = torch.tensor(actions).to(self.device)\n",
    "        rewards_v     = torch.tensor(rewards).to(self.device)\n",
    "        done_mask     = torch.BoolTensor(dones).to(self.device)\n",
    "\n",
    "        actions_v         = actions_v.unsqueeze(-1)\n",
    "        state_action_vals = self.agent.net(states_v).gather(1, actions_v)\n",
    "        state_action_vals = state_action_vals.squeeze(-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_state_vals            = self.agent.tgt_net.target_model(next_states_v).max(1)[0]\n",
    "            next_state_vals[done_mask] = 0.0\n",
    "    \n",
    "        bellman_vals = next_state_vals.detach() * self.gamma + rewards_v\n",
    "        return nn.MSELoss()(state_action_vals, bellman_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bfh_mt_hs2020_rl_basics.agent import SimpleAgent\n",
    "from bfh_mt_hs2020_rl_basics.env import CarEnv\n",
    "\n",
    "def basic_simple_init(device=torch.device(\"cpu\")) -> SimpleBridge:\n",
    "    env = CarEnv()\n",
    "    agent = SimpleAgent(env, device, gamma=0.9, buffer_size=1000)\n",
    "    bridge = SimpleBridge(agent, gamma=0.9)\n",
    "    \n",
    "    return bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_experiences() -> List[ExperienceFirstLast]:\n",
    "    return [\n",
    "        ExperienceFirstLast( np.array([0.0, 0.0, 0.0], dtype=np.float32), np.int64(0), 1.0,  np.array([0.5, 0.5, 0.5], dtype=np.float32)),\n",
    "        ExperienceFirstLast( np.array([1.0, 1.0, 1.0], dtype=np.float32), np.int64(1), 2.0,  None)        \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_init_cuda():\n",
    "    assert basic_simple_init(torch.device(\"cuda\")) != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_init_cpu():\n",
    "    assert basic_simple_init(torch.device(\"cpu\")) != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_unpack():\n",
    "    bridge = basic_simple_init()\n",
    "    batch = simple_experiences()\n",
    "    unpacked = bridge._unpack_batch(batch)\n",
    "    # todo -Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_calc_loss():\n",
    "    bridge = basic_simple_init()\n",
    "    batch = simple_experiences()\n",
    "    loss = bridge._calc_loss(batch)\n",
    "    # todo -Checks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine\n",
    "\n",
    "def test_process_batch(device=torch.device(\"cpu\")):\n",
    "    bridge = basic_simple_init(device)\n",
    "    batch = simple_experiences()\n",
    "    bridge.process_batch(Engine(bridge.process_batch), batch)\n",
    "    # todo -Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch_generator(device=torch.device(\"cpu\")):\n",
    "    # Test Iterator\n",
    "    bridge = basic_simple_init(device)\n",
    "    a = bridge.batch_generator()\n",
    "    nextbatch = next(a)\n",
    "    assert len(nextbatch) == 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basis Tests\n",
    "test_init_cpu()\n",
    "test_init_cuda()\n",
    "test_unpack()\n",
    "test_calc_loss()\n",
    "test_process_batch()\n",
    "test_batch_generator()\n",
    "test_process_batch(torch.device(\"cuda\"))\n",
    "test_batch_generator(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RainbowBridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from bfh_mt_hs2020_rl_basics.agent import AgentBase, RainbowAgent\n",
    "\n",
    "from typing import Iterable, Tuple, List\n",
    "import numpy as np\n",
    "\n",
    "from ignite.engine import Engine\n",
    "\n",
    "from ptan.experience import ExperienceFirstLast\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer, Adam\n",
    "from torch import device\n",
    "\n",
    "\n",
    "class RainbowBridge(BridgeBase):\n",
    "    \n",
    "    def __init__(self, agent: RainbowAgent, optimizer: Optimizer = None, \n",
    "                 learning_rate: float = 0.0001, \n",
    "                 gamma: float = 0.9, \n",
    "                 initial_population: int = 1000, \n",
    "                 batch_size: int = 32,\n",
    "                 beta_start: float = 0.4,\n",
    "                 beta_frames: int = 50000):\n",
    "        \n",
    "        super(RainbowBridge, self).__init__(agent, optimizer, learning_rate, gamma, initial_population, batch_size)\n",
    "\n",
    "        self.beta_start = beta_start\n",
    "        self.beta = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "\n",
    "    def get_sample(self):\n",
    "        return self.agent.buffer.sample(self.batch_size, self.beta)\n",
    "    \n",
    "    \n",
    "    def _update_beta(self, idx):\n",
    "        v = self.beta_start + idx * (1.0 - self.beta_start) / self.beta_frames\n",
    "        self.beta = min(1.0, v)\n",
    "        return self.beta\n",
    "    \n",
    "    \n",
    "    def process_batch(self, engine, batch_data):\n",
    "        \n",
    "        batch, batch_indices, batch_weights = batch_data\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        loss_v, sample_prios = self._calc_loss(\n",
    "            batch, \n",
    "            batch_weights, \n",
    "            gamma=self.gamma**self.agent.steps_count)\n",
    "        \n",
    "        loss_v.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.agent.buffer.update_priorities(batch_indices, sample_prios)\n",
    "        \n",
    "        self.agent.iteration_completed(engine.state.iteration)\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"loss\": loss_v.item(),\n",
    "            \"beta\": self._update_beta(engine.state.iteration),\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def _calc_loss(self, batch, batch_weights, gamma):\n",
    "        \n",
    "        states, actions, rewards, dones, next_states = self._unpack_batch(batch)\n",
    "\n",
    "        states_v        = torch.tensor(states).to(self.device)\n",
    "        actions_v       = torch.tensor(actions).to(self.device)\n",
    "        rewards_v       = torch.tensor(rewards).to(self.device)\n",
    "        done_mask       = torch.BoolTensor(dones).to(self.device)\n",
    "        batch_weights_v = torch.tensor(batch_weights).to(self.device)\n",
    "\n",
    "        state_action_values = self.agent.net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_states_v                = torch.tensor(next_states).to(self.device)\n",
    "            next_state_values            = self.agent.tgt_net.target_model(next_states_v).max(1)[0]\n",
    "            next_state_values[done_mask] = 0.0\n",
    "            expected_state_action_values = next_state_values.detach() * gamma + rewards_v\n",
    "            \n",
    "        losses_v = batch_weights_v * (state_action_values - expected_state_action_values) ** 2\n",
    "        \n",
    "        return losses_v.mean(), (losses_v + 1e-5).data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bfh_mt_hs2020_rl_basics.agent import RainbowAgent\n",
    "from bfh_mt_hs2020_rl_basics.env import CarEnv\n",
    "\n",
    "def basic_rainbow_init(device=torch.device(\"cpu\")) -> RainbowBridge:\n",
    "    env = CarEnv()\n",
    "    agent = RainbowAgent(env, device, gamma=0.9, buffer_size=1000)\n",
    "    bridge = RainbowBridge(agent, gamma=0.9)\n",
    "    \n",
    "    return bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_init_rainbow_cuda():\n",
    "    assert basic_rainbow_init(torch.device(\"cuda\")) != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_init_rainbow_cpu():\n",
    "    assert basic_rainbow_init(torch.device(\"cpu\")) != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rainbow_calc_loss():\n",
    "    bridge = basic_rainbow_init()\n",
    "    batch = simple_experiences()\n",
    "    loss = bridge._calc_loss(batch, [0.5,0.5], 0.9)\n",
    "    # todo -Checks  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Engine\n",
    "\n",
    "def test_rainbow_process_batch(device=torch.device(\"cpu\")):\n",
    "    bridge = basic_rainbow_init(device)\n",
    "    bridge.agent.buffer.buffer = [0,1]\n",
    "    batch = simple_experiences()\n",
    "    bridge.process_batch(Engine(bridge.process_batch), (batch, [0,1],[0.5,0.5]))\n",
    "    # todo -Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rainbow_batch_generator(device=torch.device(\"cpu\")):\n",
    "    # Test Iterator\n",
    "    bridge = basic_rainbow_init(device)\n",
    "    a = bridge.batch_generator()\n",
    "    nextbatch, idxes, weights = next(a)\n",
    "    assert len(nextbatch) == 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rainbow_batch_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_init_rainbow_cpu()\n",
    "test_init_rainbow_cuda()\n",
    "test_rainbow_calc_loss()\n",
    "test_rainbow_process_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
