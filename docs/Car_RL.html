---

title: Car RL


keywords: fastai
sidebar: home_sidebar



nb_path: "00_Car_RL.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 00_Car_RL.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">bfh_mt_hs2020_rl_basics.env</span> <span class="kn">import</span> <span class="n">CarEnv</span>
<span class="kn">from</span> <span class="nn">bfh_mt_hs2020_rl_basics.agent</span> <span class="kn">import</span> <span class="n">SimpleAgent</span><span class="p">,</span> <span class="n">RainbowAgent</span>
<span class="kn">from</span> <span class="nn">bfh_mt_hs2020_rl_basics.bridge</span> <span class="kn">import</span> <span class="n">SimpleBridge</span><span class="p">,</span> <span class="n">RainbowBridge</span>
<span class="kn">from</span> <span class="nn">bfh_mt_hs2020_rl_basics.loop</span> <span class="kn">import</span> <span class="n">LoopControl</span>

<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">gym.spaces</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Discrete</span><span class="p">,</span> <span class="n">Box</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">types</span> <span class="kn">import</span> <span class="n">SimpleNamespace</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">Adam</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">HYPERPARAMS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;base_setup&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>
        
            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>    <span class="c1"># simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
        
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>

            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>   <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>
      
            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">,</span>    <span class="c1"># simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>

            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>   <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps_2_cuda&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cuda&quot;</span><span class="p">,</span>   <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>    <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>

            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>   <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>

  <span class="s1">&#39;buffer_eps_2_cpu&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>   <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>    <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps_lr_cpu&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>
      
            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.00005</span><span class="p">,</span>  <span class="c1"># *learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
  
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps_2_cpu_limit&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># *are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>   <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>    <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;r_buffer_eps&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>
      
            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span>      <span class="c1"># *agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># *simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">3</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>

            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>   <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">})</span>    <span class="p">,</span>
  <span class="s1">&#39;r_buffer_eps_limit&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>
      
            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span>      <span class="c1"># *agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># *simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">3</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>

            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps_2_cpu_gamma&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>     <span class="c1"># *discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps_2_cpu_gamma_time&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># *Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>     <span class="c1"># *discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps_2_cpu_gamma_time_lr&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># *Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.00005</span><span class="p">,</span>  <span class="c1"># *learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>     <span class="c1"># *discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="o">-</span><span class="mf">50.0</span><span class="p">,</span>    <span class="c1"># *target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps_2_cpu_time_buffer&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># *Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># *size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>     <span class="c1"># *discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">20000</span><span class="p">,</span>    <span class="c1"># *initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">48</span><span class="p">,</span>       <span class="c1"># *batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="o">-</span><span class="mf">50.0</span><span class="p">,</span>    <span class="c1"># *target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span> 
  <span class="s1">&#39;buffer_eps_2_cpu_time_buffer_lrbg&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># *Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># *size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>    <span class="c1"># *learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>     <span class="c1"># *discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">20000</span><span class="p">,</span>    <span class="c1"># *initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">48</span><span class="p">,</span>       <span class="c1"># *batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="o">-</span><span class="mf">50.0</span><span class="p">,</span>    <span class="c1"># *target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>     
    
    <span class="c1"># ab hier mit timestep im state</span>
      <span class="s1">&#39;buffer_eps_2_cpu_gamma_time_state&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># *Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>     <span class="c1"># *discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">})</span>
<span class="p">}</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_control</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">,</span> <span class="n">config_name</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LoopControl</span><span class="p">:</span>
    
    <span class="n">env</span> <span class="o">=</span> <span class="n">CarEnv</span><span class="p">(</span><span class="n">mode_energy_penalty</span>   <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">env_mode_energy_penalty</span><span class="p">,</span> 
                 <span class="n">mode_random</span>           <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">env_mode_random</span><span class="p">,</span> 
                 <span class="n">mode_limit_steps</span>      <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">env_mode_limit_steps</span><span class="p">,</span>
                 <span class="n">mode_time_penalty</span>     <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">env_mode_time_penalty</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_type</span> <span class="o">==</span> <span class="s2">&quot;s&quot;</span><span class="p">:</span> <span class="c1"># simple agent</span>
        <span class="n">agent</span> <span class="o">=</span> <span class="n">SimpleAgent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> 
                            <span class="n">devicestr</span>  <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_device</span><span class="p">,</span> 
                            <span class="n">gamma</span>           <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_gamma_exp</span><span class="p">,</span> 
                            <span class="n">buffer_size</span>     <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_buffer_size</span><span class="p">,</span>
                            <span class="n">target_net_sync</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_target_net_sync</span><span class="p">,</span>
                            <span class="n">eps_start</span>       <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_simple_eps_start</span><span class="p">,</span>
                            <span class="n">eps_final</span>       <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_simple_eps_final</span><span class="p">,</span>
                            <span class="n">eps_frames</span>      <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_simple_eps_frames</span><span class="p">,</span>
                           <span class="p">)</span>

        <span class="n">bridge</span> <span class="o">=</span> <span class="n">SimpleBridge</span><span class="p">(</span><span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span>
                            <span class="n">optimizer</span>          <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_optimizer</span><span class="p">,</span>
                            <span class="n">learning_rate</span>      <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_learning_rate</span><span class="p">,</span>
                            <span class="n">gamma</span>              <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_gamma</span><span class="p">,</span>
                            <span class="n">initial_population</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_initial_population</span><span class="p">,</span>
                            <span class="n">batch_size</span>         <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_batch_size</span><span class="p">,</span>
                           <span class="p">)</span>
    
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_type</span> <span class="o">==</span> <span class="s2">&quot;r&quot;</span><span class="p">:</span> <span class="c1"># rainbow agent</span>
        <span class="n">agent</span> <span class="o">=</span> <span class="n">RainbowAgent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> 
                            <span class="n">devicestr</span>  <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_device</span><span class="p">,</span> 
                            <span class="n">gamma</span>              <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_gamma_exp</span><span class="p">,</span> 
                            <span class="n">buffer_size</span>        <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_buffer_size</span><span class="p">,</span>
                            <span class="n">target_net_sync</span>    <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_target_net_sync</span><span class="p">,</span>
                            <span class="n">steps_count</span>        <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_rain_steps_count</span><span class="p">,</span>
                            <span class="n">prio_replay_alpha</span>  <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_rain_prio_replay_alpha</span>
                           <span class="p">)</span>

        <span class="n">bridge</span> <span class="o">=</span> <span class="n">RainbowBridge</span><span class="p">(</span><span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span>
                            <span class="n">optimizer</span>          <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_optimizer</span><span class="p">,</span>
                            <span class="n">learning_rate</span>      <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_learning_rate</span><span class="p">,</span>
                            <span class="n">gamma</span>              <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_gamma</span><span class="p">,</span>
                            <span class="n">initial_population</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_initial_population</span><span class="p">,</span>
                            <span class="n">batch_size</span>         <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_batch_size</span><span class="p">,</span>
                            <span class="n">beta_start</span>         <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_rain_beta_start</span><span class="p">,</span>
                            <span class="n">beta_frames</span>        <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_rain_beta_frames</span>
                           <span class="p">)</span>        
    
    <span class="n">control</span> <span class="o">=</span> <span class="n">LoopControl</span><span class="p">(</span>
                   <span class="n">bridge</span>              <span class="o">=</span> <span class="n">bridge</span><span class="p">,</span> 
                   <span class="n">run_name</span>            <span class="o">=</span> <span class="n">config_name</span><span class="p">,</span> 
                   <span class="n">bound_avg_reward</span>    <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">loop_bound_avg_reward</span><span class="p">,</span>
                   <span class="n">logtb</span>               <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">loop_logtb</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">control</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">run_example</span><span class="p">(</span><span class="n">config_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="c1"># get rid of missing metrics warning</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
    
    <span class="n">control</span> <span class="o">=</span> <span class="n">create_control</span><span class="p">(</span><span class="n">HYPERPARAMS</span><span class="p">[</span><span class="n">config_name</span><span class="p">],</span> <span class="n">config_name</span><span class="p">)</span>
    <span class="n">control</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># run_example(&#39;buffer_eps&#39;)</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cuda&#39;)</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cpu&#39;)</span>
<span class="c1">#run_example(&#39;buffer_eps_lr_cpu&#39;)</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cpu_limit&#39;) # abgebrochen ... hat immer hin und hergeschwankt</span>
<span class="c1">#run_example(&#39;r_buffer_eps&#39;) # abgebrochen.. hat ziemlich lange bei einer Episode gedreht</span>
<span class="c1">#run_example(&#39;r_buffer_eps_limit&#39;) # abgebrochen</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cpu_gamma&#39;) # abgebrochen ..</span>
<span class="c1"># run_example(&#39;buffer_eps_2_cpu_gamma_time&#39;) # abgebrochen.. nach ca. 40 minuten</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cpu_gamma_time_lr&#39;) #abgebrochen Achtung: Graf hat zeitlcken</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cpu_time_buffer&#39;) #buffersize doppelt, init auf 20&#39;000, Batchsize 48 -&gt; idee -&gt; ausgogener -&gt; abgebrochen: am Anfang gut uns stabiler</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cpu_time_buffer_lrbg&#39;)# abgebrochen </span>
<span class="n">run_example</span><span class="p">(</span><span class="s1">&#39;buffer_eps_2_cpu_gamma_time_state&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Current run is terminating due to exception: size mismatch, m1: [1 x 4], m2: [3 x 128] at ..\aten\src\TH/generic/THTensorMath.cpp:41.
Engine run is terminating due to exception: size mismatch, m1: [1 x 4], m2: [3 x 128] at ..\aten\src\TH/generic/THTensorMath.cpp:41.
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-intense-fg ansi-bold">---------------------------------------------------------------------------</span>
<span class="ansi-red-intense-fg ansi-bold">RuntimeError</span>                              Traceback (most recent call last)
<span class="ansi-green-intense-fg ansi-bold">&lt;ipython-input-37-b7f3ff74227b&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span>
<span class="ansi-green-fg">     12</span> <span class="ansi-red-intense-fg ansi-bold">#run_example(&#39;buffer_eps_2_cpu_time_buffer&#39;) #buffersize doppelt, init auf 20&#39;000, Batchsize 48 -&gt; idee -&gt; ausgogener -&gt; abgebrochen: am Anfang gut uns stabiler</span>
<span class="ansi-green-fg">     13</span> <span class="ansi-red-intense-fg ansi-bold">#run_example(&#39;buffer_eps_2_cpu_time_buffer_lrbg&#39;)# abgebrochen</span>
<span class="ansi-green-intense-fg ansi-bold">---&gt; 14</span><span class="ansi-yellow-intense-fg ansi-bold"> </span>run_example<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-blue-intense-fg ansi-bold">&#39;buffer_eps_2_cpu_gamma_time_state&#39;</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>

<span class="ansi-green-intense-fg ansi-bold">&lt;ipython-input-36-3d95ee1a734b&gt;</span> in <span class="ansi-cyan-fg">run_example</span><span class="ansi-blue-intense-fg ansi-bold">(config_name)</span>
<span class="ansi-green-fg">      4</span> 
<span class="ansi-green-fg">      5</span>     control <span class="ansi-yellow-intense-fg ansi-bold">=</span> create_control<span class="ansi-yellow-intense-fg ansi-bold">(</span>HYPERPARAMS<span class="ansi-yellow-intense-fg ansi-bold">[</span>config_name<span class="ansi-yellow-intense-fg ansi-bold">]</span><span class="ansi-yellow-intense-fg ansi-bold">,</span> config_name<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">----&gt; 6</span><span class="ansi-yellow-intense-fg ansi-bold">     </span>control<span class="ansi-yellow-intense-fg ansi-bold">.</span>run<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\projects\bfh_mt_hs2020_rl_basics\bfh_mt_hs2020_rl_basics\loop.py</span> in <span class="ansi-cyan-fg">run</span><span class="ansi-blue-intense-fg ansi-bold">(self)</span>
<span class="ansi-green-fg">     79</span> 
<span class="ansi-green-fg">     80</span>     <span class="ansi-green-intense-fg ansi-bold">def</span> run<span class="ansi-yellow-intense-fg ansi-bold">(</span>self<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">---&gt; 81</span><span class="ansi-yellow-intense-fg ansi-bold">         </span>self<span class="ansi-yellow-intense-fg ansi-bold">.</span>engine<span class="ansi-yellow-intense-fg ansi-bold">.</span>run<span class="ansi-yellow-intense-fg ansi-bold">(</span>self<span class="ansi-yellow-intense-fg ansi-bold">.</span>bridge<span class="ansi-yellow-intense-fg ansi-bold">.</span>batch_generator<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\ignite\engine\engine.py</span> in <span class="ansi-cyan-fg">run</span><span class="ansi-blue-intense-fg ansi-bold">(self, data, max_epochs, epoch_length, seed)</span>
<span class="ansi-green-fg">    689</span> 
<span class="ansi-green-fg">    690</span>         self<span class="ansi-yellow-intense-fg ansi-bold">.</span>state<span class="ansi-yellow-intense-fg ansi-bold">.</span>dataloader <span class="ansi-yellow-intense-fg ansi-bold">=</span> data
<span class="ansi-green-intense-fg ansi-bold">--&gt; 691</span><span class="ansi-yellow-intense-fg ansi-bold">         </span><span class="ansi-green-intense-fg ansi-bold">return</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_internal_run<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    692</span> 
<span class="ansi-green-fg">    693</span>     <span class="ansi-yellow-intense-fg ansi-bold">@</span>staticmethod

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\ignite\engine\engine.py</span> in <span class="ansi-cyan-fg">_internal_run</span><span class="ansi-blue-intense-fg ansi-bold">(self)</span>
<span class="ansi-green-fg">    760</span>             self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_dataloader_iter <span class="ansi-yellow-intense-fg ansi-bold">=</span> <span class="ansi-green-intense-fg ansi-bold">None</span>
<span class="ansi-green-fg">    761</span>             self<span class="ansi-yellow-intense-fg ansi-bold">.</span>logger<span class="ansi-yellow-intense-fg ansi-bold">.</span>error<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-blue-intense-fg ansi-bold">&#34;Engine run is terminating due to exception: %s.&#34;</span><span class="ansi-yellow-intense-fg ansi-bold">,</span> str<span class="ansi-yellow-intense-fg ansi-bold">(</span>e<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">--&gt; 762</span><span class="ansi-yellow-intense-fg ansi-bold">             </span>self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_handle_exception<span class="ansi-yellow-intense-fg ansi-bold">(</span>e<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    763</span> 
<span class="ansi-green-fg">    764</span>         self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_dataloader_iter <span class="ansi-yellow-intense-fg ansi-bold">=</span> <span class="ansi-green-intense-fg ansi-bold">None</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\ignite\engine\engine.py</span> in <span class="ansi-cyan-fg">_handle_exception</span><span class="ansi-blue-intense-fg ansi-bold">(self, e)</span>
<span class="ansi-green-fg">    465</span>             self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_fire_event<span class="ansi-yellow-intense-fg ansi-bold">(</span>Events<span class="ansi-yellow-intense-fg ansi-bold">.</span>EXCEPTION_RAISED<span class="ansi-yellow-intense-fg ansi-bold">,</span> e<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    466</span>         <span class="ansi-green-intense-fg ansi-bold">else</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">--&gt; 467</span><span class="ansi-yellow-intense-fg ansi-bold">             </span><span class="ansi-green-intense-fg ansi-bold">raise</span> e
<span class="ansi-green-fg">    468</span> 
<span class="ansi-green-fg">    469</span>     <span class="ansi-yellow-intense-fg ansi-bold">@</span>property

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\ignite\engine\engine.py</span> in <span class="ansi-cyan-fg">_internal_run</span><span class="ansi-blue-intense-fg ansi-bold">(self)</span>
<span class="ansi-green-fg">    728</span>                     self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_setup_engine<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    729</span> 
<span class="ansi-green-intense-fg ansi-bold">--&gt; 730</span><span class="ansi-yellow-intense-fg ansi-bold">                 </span>time_taken <span class="ansi-yellow-intense-fg ansi-bold">=</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_run_once_on_dataset<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    731</span>                 <span class="ansi-red-intense-fg ansi-bold"># time is available for handlers but must be update after fire</span>
<span class="ansi-green-fg">    732</span>                 self<span class="ansi-yellow-intense-fg ansi-bold">.</span>state<span class="ansi-yellow-intense-fg ansi-bold">.</span>times<span class="ansi-yellow-intense-fg ansi-bold">[</span>Events<span class="ansi-yellow-intense-fg ansi-bold">.</span>EPOCH_COMPLETED<span class="ansi-yellow-intense-fg ansi-bold">.</span>name<span class="ansi-yellow-intense-fg ansi-bold">]</span> <span class="ansi-yellow-intense-fg ansi-bold">=</span> time_taken

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\ignite\engine\engine.py</span> in <span class="ansi-cyan-fg">_run_once_on_dataset</span><span class="ansi-blue-intense-fg ansi-bold">(self)</span>
<span class="ansi-green-fg">    826</span>         <span class="ansi-green-intense-fg ansi-bold">except</span> Exception <span class="ansi-green-intense-fg ansi-bold">as</span> e<span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">    827</span>             self<span class="ansi-yellow-intense-fg ansi-bold">.</span>logger<span class="ansi-yellow-intense-fg ansi-bold">.</span>error<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-blue-intense-fg ansi-bold">&#34;Current run is terminating due to exception: %s.&#34;</span><span class="ansi-yellow-intense-fg ansi-bold">,</span> str<span class="ansi-yellow-intense-fg ansi-bold">(</span>e<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">--&gt; 828</span><span class="ansi-yellow-intense-fg ansi-bold">             </span>self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_handle_exception<span class="ansi-yellow-intense-fg ansi-bold">(</span>e<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    829</span> 
<span class="ansi-green-fg">    830</span>         <span class="ansi-green-intense-fg ansi-bold">return</span> time<span class="ansi-yellow-intense-fg ansi-bold">.</span>time<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span> <span class="ansi-yellow-intense-fg ansi-bold">-</span> start_time

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\ignite\engine\engine.py</span> in <span class="ansi-cyan-fg">_handle_exception</span><span class="ansi-blue-intense-fg ansi-bold">(self, e)</span>
<span class="ansi-green-fg">    465</span>             self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_fire_event<span class="ansi-yellow-intense-fg ansi-bold">(</span>Events<span class="ansi-yellow-intense-fg ansi-bold">.</span>EXCEPTION_RAISED<span class="ansi-yellow-intense-fg ansi-bold">,</span> e<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    466</span>         <span class="ansi-green-intense-fg ansi-bold">else</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">--&gt; 467</span><span class="ansi-yellow-intense-fg ansi-bold">             </span><span class="ansi-green-intense-fg ansi-bold">raise</span> e
<span class="ansi-green-fg">    468</span> 
<span class="ansi-green-fg">    469</span>     <span class="ansi-yellow-intense-fg ansi-bold">@</span>property

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\ignite\engine\engine.py</span> in <span class="ansi-cyan-fg">_run_once_on_dataset</span><span class="ansi-blue-intense-fg ansi-bold">(self)</span>
<span class="ansi-green-fg">    777</span>                     <span class="ansi-green-intense-fg ansi-bold">if</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>last_event_name <span class="ansi-yellow-intense-fg ansi-bold">!=</span> Events<span class="ansi-yellow-intense-fg ansi-bold">.</span>DATALOADER_STOP_ITERATION<span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">    778</span>                         self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_fire_event<span class="ansi-yellow-intense-fg ansi-bold">(</span>Events<span class="ansi-yellow-intense-fg ansi-bold">.</span>GET_BATCH_STARTED<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">--&gt; 779</span><span class="ansi-yellow-intense-fg ansi-bold">                     </span>self<span class="ansi-yellow-intense-fg ansi-bold">.</span>state<span class="ansi-yellow-intense-fg ansi-bold">.</span>batch <span class="ansi-yellow-intense-fg ansi-bold">=</span> next<span class="ansi-yellow-intense-fg ansi-bold">(</span>self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_dataloader_iter<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    780</span>                     self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_fire_event<span class="ansi-yellow-intense-fg ansi-bold">(</span>Events<span class="ansi-yellow-intense-fg ansi-bold">.</span>GET_BATCH_COMPLETED<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    781</span>                     iter_counter <span class="ansi-yellow-intense-fg ansi-bold">+=</span> <span class="ansi-cyan-intense-fg ansi-bold">1</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\projects\bfh_mt_hs2020_rl_basics\bfh_mt_hs2020_rl_basics\bridge.py</span> in <span class="ansi-cyan-fg">batch_generator</span><span class="ansi-blue-intense-fg ansi-bold">(self)</span>
<span class="ansi-green-fg">     40</span> 
<span class="ansi-green-fg">     41</span>     <span class="ansi-green-intense-fg ansi-bold">def</span> batch_generator<span class="ansi-yellow-intense-fg ansi-bold">(</span>self<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">---&gt; 42</span><span class="ansi-yellow-intense-fg ansi-bold">         </span>self<span class="ansi-yellow-intense-fg ansi-bold">.</span>agent<span class="ansi-yellow-intense-fg ansi-bold">.</span>buffer<span class="ansi-yellow-intense-fg ansi-bold">.</span>populate<span class="ansi-yellow-intense-fg ansi-bold">(</span>self<span class="ansi-yellow-intense-fg ansi-bold">.</span>initial_population<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">     43</span>         <span class="ansi-green-intense-fg ansi-bold">while</span> <span class="ansi-green-intense-fg ansi-bold">True</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">     44</span>             self<span class="ansi-yellow-intense-fg ansi-bold">.</span>agent<span class="ansi-yellow-intense-fg ansi-bold">.</span>buffer<span class="ansi-yellow-intense-fg ansi-bold">.</span>populate<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-cyan-intense-fg ansi-bold">1</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\ptan-0.6-py3.7.egg\ptan\experience.py</span> in <span class="ansi-cyan-fg">populate</span><span class="ansi-blue-intense-fg ansi-bold">(self, samples)</span>
<span class="ansi-green-fg">    366</span>         &#34;&#34;&#34;
<span class="ansi-green-fg">    367</span>         <span class="ansi-green-intense-fg ansi-bold">for</span> _ <span class="ansi-green-intense-fg ansi-bold">in</span> range<span class="ansi-yellow-intense-fg ansi-bold">(</span>samples<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">--&gt; 368</span><span class="ansi-yellow-intense-fg ansi-bold">             </span>entry <span class="ansi-yellow-intense-fg ansi-bold">=</span> next<span class="ansi-yellow-intense-fg ansi-bold">(</span>self<span class="ansi-yellow-intense-fg ansi-bold">.</span>experience_source_iter<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    369</span>             self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_add<span class="ansi-yellow-intense-fg ansi-bold">(</span>entry<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    370</span> 

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\ptan-0.6-py3.7.egg\ptan\experience.py</span> in <span class="ansi-cyan-fg">__iter__</span><span class="ansi-blue-intense-fg ansi-bold">(self)</span>
<span class="ansi-green-fg">    174</span> 
<span class="ansi-green-fg">    175</span>     <span class="ansi-green-intense-fg ansi-bold">def</span> __iter__<span class="ansi-yellow-intense-fg ansi-bold">(</span>self<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">--&gt; 176</span><span class="ansi-yellow-intense-fg ansi-bold">         </span><span class="ansi-green-intense-fg ansi-bold">for</span> exp <span class="ansi-green-intense-fg ansi-bold">in</span> super<span class="ansi-yellow-intense-fg ansi-bold">(</span>ExperienceSourceFirstLast<span class="ansi-yellow-intense-fg ansi-bold">,</span> self<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">.</span>__iter__<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">    177</span>             <span class="ansi-green-intense-fg ansi-bold">if</span> exp<span class="ansi-yellow-intense-fg ansi-bold">[</span><span class="ansi-yellow-intense-fg ansi-bold">-</span><span class="ansi-cyan-intense-fg ansi-bold">1</span><span class="ansi-yellow-intense-fg ansi-bold">]</span><span class="ansi-yellow-intense-fg ansi-bold">.</span>done <span class="ansi-green-intense-fg ansi-bold">and</span> len<span class="ansi-yellow-intense-fg ansi-bold">(</span>exp<span class="ansi-yellow-intense-fg ansi-bold">)</span> <span class="ansi-yellow-intense-fg ansi-bold">&lt;=</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>steps<span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">    178</span>                 last_state <span class="ansi-yellow-intense-fg ansi-bold">=</span> <span class="ansi-green-intense-fg ansi-bold">None</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\ptan-0.6-py3.7.egg\ptan\experience.py</span> in <span class="ansi-cyan-fg">__iter__</span><span class="ansi-blue-intense-fg ansi-bold">(self)</span>
<span class="ansi-green-fg">     80</span>                     states_indices<span class="ansi-yellow-intense-fg ansi-bold">.</span>append<span class="ansi-yellow-intense-fg ansi-bold">(</span>idx<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">     81</span>             <span class="ansi-green-intense-fg ansi-bold">if</span> states_input<span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">---&gt; 82</span><span class="ansi-yellow-intense-fg ansi-bold">                 </span>states_actions<span class="ansi-yellow-intense-fg ansi-bold">,</span> new_agent_states <span class="ansi-yellow-intense-fg ansi-bold">=</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>agent<span class="ansi-yellow-intense-fg ansi-bold">(</span>states_input<span class="ansi-yellow-intense-fg ansi-bold">,</span> agent_states<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">     83</span>                 <span class="ansi-green-intense-fg ansi-bold">for</span> idx<span class="ansi-yellow-intense-fg ansi-bold">,</span> action <span class="ansi-green-intense-fg ansi-bold">in</span> enumerate<span class="ansi-yellow-intense-fg ansi-bold">(</span>states_actions<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">     84</span>                     g_idx <span class="ansi-yellow-intense-fg ansi-bold">=</span> states_indices<span class="ansi-yellow-intense-fg ansi-bold">[</span>idx<span class="ansi-yellow-intense-fg ansi-bold">]</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\torch\autograd\grad_mode.py</span> in <span class="ansi-cyan-fg">decorate_context</span><span class="ansi-blue-intense-fg ansi-bold">(*args, **kwargs)</span>
<span class="ansi-green-fg">     13</span>         <span class="ansi-green-intense-fg ansi-bold">def</span> decorate_context<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">*</span>args<span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-yellow-intense-fg ansi-bold">**</span>kwargs<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">     14</span>             <span class="ansi-green-intense-fg ansi-bold">with</span> self<span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">---&gt; 15</span><span class="ansi-yellow-intense-fg ansi-bold">                 </span><span class="ansi-green-intense-fg ansi-bold">return</span> func<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">*</span>args<span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-yellow-intense-fg ansi-bold">**</span>kwargs<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">     16</span>         <span class="ansi-green-intense-fg ansi-bold">return</span> decorate_context
<span class="ansi-green-fg">     17</span> 

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\ptan-0.6-py3.7.egg\ptan\agent.py</span> in <span class="ansi-cyan-fg">__call__</span><span class="ansi-blue-intense-fg ansi-bold">(self, states, agent_states)</span>
<span class="ansi-green-fg">     72</span>             <span class="ansi-green-intense-fg ansi-bold">if</span> torch<span class="ansi-yellow-intense-fg ansi-bold">.</span>is_tensor<span class="ansi-yellow-intense-fg ansi-bold">(</span>states<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">     73</span>                 states <span class="ansi-yellow-intense-fg ansi-bold">=</span> states<span class="ansi-yellow-intense-fg ansi-bold">.</span>to<span class="ansi-yellow-intense-fg ansi-bold">(</span>self<span class="ansi-yellow-intense-fg ansi-bold">.</span>device<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-intense-fg ansi-bold">---&gt; 74</span><span class="ansi-yellow-intense-fg ansi-bold">         </span>q_v <span class="ansi-yellow-intense-fg ansi-bold">=</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>dqn_model<span class="ansi-yellow-intense-fg ansi-bold">(</span>states<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">     75</span>         q <span class="ansi-yellow-intense-fg ansi-bold">=</span> q_v<span class="ansi-yellow-intense-fg ansi-bold">.</span>data<span class="ansi-yellow-intense-fg ansi-bold">.</span>cpu<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">.</span>numpy<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">     76</span>         actions <span class="ansi-yellow-intense-fg ansi-bold">=</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>action_selector<span class="ansi-yellow-intense-fg ansi-bold">(</span>q<span class="ansi-yellow-intense-fg ansi-bold">)</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\torch\nn\modules\module.py</span> in <span class="ansi-cyan-fg">_call_impl</span><span class="ansi-blue-intense-fg ansi-bold">(self, *input, **kwargs)</span>
<span class="ansi-green-fg">    720</span>             result <span class="ansi-yellow-intense-fg ansi-bold">=</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_slow_forward<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">*</span>input<span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-yellow-intense-fg ansi-bold">**</span>kwargs<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    721</span>         <span class="ansi-green-intense-fg ansi-bold">else</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">--&gt; 722</span><span class="ansi-yellow-intense-fg ansi-bold">             </span>result <span class="ansi-yellow-intense-fg ansi-bold">=</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>forward<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">*</span>input<span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-yellow-intense-fg ansi-bold">**</span>kwargs<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    723</span>         for hook in itertools.chain(
<span class="ansi-green-fg">    724</span>                 _global_forward_hooks<span class="ansi-yellow-intense-fg ansi-bold">.</span>values<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">,</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\projects\bfh_mt_hs2020_rl_basics\bfh_mt_hs2020_rl_basics\agent.py</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-intense-fg ansi-bold">(self, x)</span>
<span class="ansi-green-fg">     48</span> 
<span class="ansi-green-fg">     49</span>     <span class="ansi-green-intense-fg ansi-bold">def</span> forward<span class="ansi-yellow-intense-fg ansi-bold">(</span>self<span class="ansi-yellow-intense-fg ansi-bold">,</span> x<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">---&gt; 50</span><span class="ansi-yellow-intense-fg ansi-bold">         </span><span class="ansi-green-intense-fg ansi-bold">return</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>net<span class="ansi-yellow-intense-fg ansi-bold">(</span>x<span class="ansi-yellow-intense-fg ansi-bold">.</span>float<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">     51</span> 
<span class="ansi-green-fg">     52</span> <span class="ansi-red-intense-fg ansi-bold"># Cell</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\torch\nn\modules\module.py</span> in <span class="ansi-cyan-fg">_call_impl</span><span class="ansi-blue-intense-fg ansi-bold">(self, *input, **kwargs)</span>
<span class="ansi-green-fg">    720</span>             result <span class="ansi-yellow-intense-fg ansi-bold">=</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_slow_forward<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">*</span>input<span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-yellow-intense-fg ansi-bold">**</span>kwargs<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    721</span>         <span class="ansi-green-intense-fg ansi-bold">else</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">--&gt; 722</span><span class="ansi-yellow-intense-fg ansi-bold">             </span>result <span class="ansi-yellow-intense-fg ansi-bold">=</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>forward<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">*</span>input<span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-yellow-intense-fg ansi-bold">**</span>kwargs<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    723</span>         for hook in itertools.chain(
<span class="ansi-green-fg">    724</span>                 _global_forward_hooks<span class="ansi-yellow-intense-fg ansi-bold">.</span>values<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">,</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\torch\nn\modules\container.py</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-intense-fg ansi-bold">(self, input)</span>
<span class="ansi-green-fg">    115</span>     <span class="ansi-green-intense-fg ansi-bold">def</span> forward<span class="ansi-yellow-intense-fg ansi-bold">(</span>self<span class="ansi-yellow-intense-fg ansi-bold">,</span> input<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">    116</span>         <span class="ansi-green-intense-fg ansi-bold">for</span> module <span class="ansi-green-intense-fg ansi-bold">in</span> self<span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">--&gt; 117</span><span class="ansi-yellow-intense-fg ansi-bold">             </span>input <span class="ansi-yellow-intense-fg ansi-bold">=</span> module<span class="ansi-yellow-intense-fg ansi-bold">(</span>input<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    118</span>         <span class="ansi-green-intense-fg ansi-bold">return</span> input
<span class="ansi-green-fg">    119</span> 

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\torch\nn\modules\module.py</span> in <span class="ansi-cyan-fg">_call_impl</span><span class="ansi-blue-intense-fg ansi-bold">(self, *input, **kwargs)</span>
<span class="ansi-green-fg">    720</span>             result <span class="ansi-yellow-intense-fg ansi-bold">=</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>_slow_forward<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">*</span>input<span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-yellow-intense-fg ansi-bold">**</span>kwargs<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    721</span>         <span class="ansi-green-intense-fg ansi-bold">else</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">--&gt; 722</span><span class="ansi-yellow-intense-fg ansi-bold">             </span>result <span class="ansi-yellow-intense-fg ansi-bold">=</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>forward<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">*</span>input<span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-yellow-intense-fg ansi-bold">**</span>kwargs<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">    723</span>         for hook in itertools.chain(
<span class="ansi-green-fg">    724</span>                 _global_forward_hooks<span class="ansi-yellow-intense-fg ansi-bold">.</span>values<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">,</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\torch\nn\modules\linear.py</span> in <span class="ansi-cyan-fg">forward</span><span class="ansi-blue-intense-fg ansi-bold">(self, input)</span>
<span class="ansi-green-fg">     89</span> 
<span class="ansi-green-fg">     90</span>     <span class="ansi-green-intense-fg ansi-bold">def</span> forward<span class="ansi-yellow-intense-fg ansi-bold">(</span>self<span class="ansi-yellow-intense-fg ansi-bold">,</span> input<span class="ansi-yellow-intense-fg ansi-bold">:</span> Tensor<span class="ansi-yellow-intense-fg ansi-bold">)</span> <span class="ansi-yellow-intense-fg ansi-bold">-&gt;</span> Tensor<span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">---&gt; 91</span><span class="ansi-yellow-intense-fg ansi-bold">         </span><span class="ansi-green-intense-fg ansi-bold">return</span> F<span class="ansi-yellow-intense-fg ansi-bold">.</span>linear<span class="ansi-yellow-intense-fg ansi-bold">(</span>input<span class="ansi-yellow-intense-fg ansi-bold">,</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>weight<span class="ansi-yellow-intense-fg ansi-bold">,</span> self<span class="ansi-yellow-intense-fg ansi-bold">.</span>bias<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">     92</span> 
<span class="ansi-green-fg">     93</span>     <span class="ansi-green-intense-fg ansi-bold">def</span> extra_repr<span class="ansi-yellow-intense-fg ansi-bold">(</span>self<span class="ansi-yellow-intense-fg ansi-bold">)</span> <span class="ansi-yellow-intense-fg ansi-bold">-&gt;</span> str<span class="ansi-yellow-intense-fg ansi-bold">:</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ieu\Anaconda3\envs\deep_rl_hands_on\lib\site-packages\torch\nn\functional.py</span> in <span class="ansi-cyan-fg">linear</span><span class="ansi-blue-intense-fg ansi-bold">(input, weight, bias)</span>
<span class="ansi-green-fg">   1672</span>     <span class="ansi-green-intense-fg ansi-bold">if</span> input<span class="ansi-yellow-intense-fg ansi-bold">.</span>dim<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span> <span class="ansi-yellow-intense-fg ansi-bold">==</span> <span class="ansi-cyan-intense-fg ansi-bold">2</span> <span class="ansi-green-intense-fg ansi-bold">and</span> bias <span class="ansi-green-intense-fg ansi-bold">is</span> <span class="ansi-green-intense-fg ansi-bold">not</span> <span class="ansi-green-intense-fg ansi-bold">None</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">   1673</span>         <span class="ansi-red-intense-fg ansi-bold"># fused op is marginally faster</span>
<span class="ansi-green-intense-fg ansi-bold">-&gt; 1674</span><span class="ansi-yellow-intense-fg ansi-bold">         </span>ret <span class="ansi-yellow-intense-fg ansi-bold">=</span> torch<span class="ansi-yellow-intense-fg ansi-bold">.</span>addmm<span class="ansi-yellow-intense-fg ansi-bold">(</span>bias<span class="ansi-yellow-intense-fg ansi-bold">,</span> input<span class="ansi-yellow-intense-fg ansi-bold">,</span> weight<span class="ansi-yellow-intense-fg ansi-bold">.</span>t<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">   1675</span>     <span class="ansi-green-intense-fg ansi-bold">else</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">   1676</span>         output <span class="ansi-yellow-intense-fg ansi-bold">=</span> input<span class="ansi-yellow-intense-fg ansi-bold">.</span>matmul<span class="ansi-yellow-intense-fg ansi-bold">(</span>weight<span class="ansi-yellow-intense-fg ansi-bold">.</span>t<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">)</span>

<span class="ansi-red-intense-fg ansi-bold">RuntimeError</span>: size mismatch, m1: [1 x 4], m2: [3 x 128] at ..\aten\src\TH/generic/THTensorMath.cpp:41</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

