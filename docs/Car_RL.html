---

title: Car RL


keywords: fastai
sidebar: home_sidebar



nb_path: "00_Car_RL.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 00_Car_RL.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">bfh_mt_hs2020_rl_basics.env</span> <span class="kn">import</span> <span class="n">CarEnv</span>
<span class="kn">from</span> <span class="nn">bfh_mt_hs2020_rl_basics.agent</span> <span class="kn">import</span> <span class="n">SimpleAgent</span><span class="p">,</span> <span class="n">RainbowAgent</span>
<span class="kn">from</span> <span class="nn">bfh_mt_hs2020_rl_basics.bridge</span> <span class="kn">import</span> <span class="n">SimpleBridge</span><span class="p">,</span> <span class="n">RainbowBridge</span>
<span class="kn">from</span> <span class="nn">bfh_mt_hs2020_rl_basics.loop</span> <span class="kn">import</span> <span class="n">LoopControl</span>

<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">gym.spaces</span> <span class="kn">import</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Discrete</span><span class="p">,</span> <span class="n">Box</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">types</span> <span class="kn">import</span> <span class="n">SimpleNamespace</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Optimizer</span><span class="p">,</span> <span class="n">Adam</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">HYPERPARAMS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;base_setup&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>
        
            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>    <span class="c1"># simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
        
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>

            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>   <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>
      
            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">,</span>    <span class="c1"># simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>

            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>   <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps_2_cuda&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cuda&quot;</span><span class="p">,</span>   <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>    <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>

            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>   <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>

  <span class="s1">&#39;buffer_eps_2_cpu&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>   <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>    <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps_lr_cpu&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>
      
            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.00005</span><span class="p">,</span>  <span class="c1"># *learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
  
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps_2_cpu_limit&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># *are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>   <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>    <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;r_buffer_eps&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>
      
            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span>      <span class="c1"># *agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># *simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">3</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>

            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>   <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">})</span>    <span class="p">,</span>
  <span class="s1">&#39;r_buffer_eps_limit&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>
      
            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span>      <span class="c1"># *agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># *simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">3</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>

            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps_2_cpu_gamma&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>     <span class="c1"># *discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps_2_cpu_gamma_time&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># *Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>     <span class="c1"># *discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps_2_cpu_gamma_time_lr&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># *Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.00005</span><span class="p">,</span>  <span class="c1"># *learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>     <span class="c1"># *discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="o">-</span><span class="mf">50.0</span><span class="p">,</span>    <span class="c1"># *target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
  <span class="s1">&#39;buffer_eps_2_cpu_time_buffer&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># *Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># *size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>     <span class="c1"># *discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">20000</span><span class="p">,</span>    <span class="c1"># *initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">48</span><span class="p">,</span>       <span class="c1"># *batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="o">-</span><span class="mf">50.0</span><span class="p">,</span>    <span class="c1"># *target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span> 
  <span class="s1">&#39;buffer_eps_2_cpu_time_buffer_lrbg&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># *Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># *size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>    <span class="c1"># *learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>     <span class="c1"># *discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">20000</span><span class="p">,</span>    <span class="c1"># *initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">48</span><span class="p">,</span>       <span class="c1"># *batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="o">-</span><span class="mf">50.0</span><span class="p">,</span>    <span class="c1"># *target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>     
    
    <span class="c1"># ab hier mit timestep im state</span>
      <span class="s1">&#39;buffer_eps_2_cpu_gamma_time_state&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># *Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>    <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.99</span><span class="p">,</span>     <span class="c1"># *discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
    <span class="s1">&#39;buffer_eps_2_cpu_timestate&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>   <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>    <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
    <span class="s1">&#39;buffer_eps_2_cpu_timestate_newHiddenL&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>   <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>    <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
    
    <span class="s1">&#39;buffer_eps_2_cpu_timestate_newHiddenL&#39;</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">(</span><span class="o">**</span><span class="p">{</span>
            <span class="c1"># env</span>
            <span class="s1">&#39;env_mode_energy_penalty&#39;</span>     <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># should there be a -1 point penalty for a used energy unit</span>
            <span class="s1">&#39;env_mode_random&#39;</span>             <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># does acceleration and decelartion have a random part</span>
            <span class="s1">&#39;env_mode_limit_steps&#39;</span>        <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># are the maximum possible steps limited</span>
            <span class="s1">&#39;env_mode_time_penalty&#39;</span>       <span class="p">:</span> <span class="kc">False</span><span class="p">,</span>    <span class="c1"># Penalty for every timestep</span>

            <span class="c1"># agent</span>
            <span class="s1">&#39;agent_type&#39;</span>                  <span class="p">:</span> <span class="s2">&quot;s&quot;</span><span class="p">,</span>      <span class="c1"># agent type: s=simple, r=rainbow</span>
            <span class="s1">&#39;agent_device&#39;</span>                <span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>   <span class="c1"># * cpu or cuda</span>
            <span class="s1">&#39;agent_gamma_exp&#39;</span>             <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for experience_first_last.. shouldn&#39;t matter since step_size is only 1</span>
            <span class="s1">&#39;agent_buffer_size&#39;</span>           <span class="p">:</span> <span class="mi">50000</span><span class="p">,</span>    <span class="c1"># size of replay buffer</span>
            <span class="s1">&#39;agent_target_net_sync&#39;</span>       <span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>     <span class="c1"># sync TargetNet with weights of DNN every .. iterations</span>
            <span class="s1">&#39;agent_simple_eps_start&#39;</span>      <span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>      <span class="c1"># simpleagent: epsilon start</span>
            <span class="s1">&#39;agent_simple_eps_final&#39;</span>      <span class="p">:</span> <span class="mf">0.02</span><span class="p">,</span>     <span class="c1"># simpleagent: epsilon end</span>
            <span class="s1">&#39;agent_simple_eps_frames&#39;</span>     <span class="p">:</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span>    <span class="c1"># * simpleagent: epsilon frames -&gt; how many frames until 0.02 should be reached .. decay is linear</span>
            <span class="s1">&#39;agent_rain_steps_count&#39;</span>      <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>        <span class="c1"># rainbowagent: steps per iteration</span>
            <span class="s1">&#39;agent_rain_prio_replay_alpha&#39;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>      <span class="c1"># rainbowagent: prio buffer alpha  </span>
  
            <span class="c1"># bridge  </span>
            <span class="s1">&#39;bridge_optimizer&#39;</span>            <span class="p">:</span> <span class="kc">None</span><span class="p">,</span>     <span class="c1"># Optimizer -&gt; default ist Adam</span>
            <span class="s1">&#39;bridge_learning_rate&#39;</span>        <span class="p">:</span> <span class="mf">0.0001</span><span class="p">,</span>   <span class="c1"># learningrate</span>
            <span class="s1">&#39;bridge_gamma&#39;</span>                <span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>      <span class="c1"># discount_factor for reward</span>
            <span class="s1">&#39;bridge_initial_population&#39;</span>   <span class="p">:</span> <span class="mi">5000</span><span class="p">,</span>     <span class="c1"># initial number of experiences in buffer</span>
            <span class="s1">&#39;bridge_batch_size&#39;</span>           <span class="p">:</span> <span class="mi">32</span><span class="p">,</span>       <span class="c1"># batch_size for training</span>
            <span class="s1">&#39;bridge_rain_beta_start&#39;</span>      <span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>      <span class="c1"># rainbow: start beta for Gewichtung buffer</span>
            <span class="s1">&#39;bridge_rain_beta_frames&#39;</span>     <span class="p">:</span> <span class="mi">100000</span><span class="p">,</span>   <span class="c1"># rainbow: iteration when bea reaches 1</span>
    
            <span class="c1"># loop control  </span>
            <span class="s1">&#39;loop_bound_avg_reward&#39;</span>       <span class="p">:</span> <span class="mf">50.0</span><span class="p">,</span>     <span class="c1"># target avg reward</span>
            <span class="s1">&#39;loop_logtb&#39;</span>                  <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>     <span class="c1"># Log to Tensorboard Logfile</span>
    <span class="p">}),</span>
    
<span class="p">}</span>
    
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">create_control</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">SimpleNamespace</span><span class="p">,</span> <span class="n">config_name</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LoopControl</span><span class="p">:</span>
    
    <span class="n">env</span> <span class="o">=</span> <span class="n">CarEnv</span><span class="p">(</span><span class="n">mode_energy_penalty</span>   <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">env_mode_energy_penalty</span><span class="p">,</span> 
                 <span class="n">mode_random</span>           <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">env_mode_random</span><span class="p">,</span> 
                 <span class="n">mode_limit_steps</span>      <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">env_mode_limit_steps</span><span class="p">,</span>
                 <span class="n">mode_time_penalty</span>     <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">env_mode_time_penalty</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_type</span> <span class="o">==</span> <span class="s2">&quot;s&quot;</span><span class="p">:</span> <span class="c1"># simple agent</span>
        <span class="n">agent</span> <span class="o">=</span> <span class="n">SimpleAgent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> 
                            <span class="n">devicestr</span>  <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_device</span><span class="p">,</span> 
                            <span class="n">gamma</span>           <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_gamma_exp</span><span class="p">,</span> 
                            <span class="n">buffer_size</span>     <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_buffer_size</span><span class="p">,</span>
                            <span class="n">target_net_sync</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_target_net_sync</span><span class="p">,</span>
                            <span class="n">eps_start</span>       <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_simple_eps_start</span><span class="p">,</span>
                            <span class="n">eps_final</span>       <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_simple_eps_final</span><span class="p">,</span>
                            <span class="n">eps_frames</span>      <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_simple_eps_frames</span><span class="p">,</span>
                           <span class="p">)</span>

        <span class="n">bridge</span> <span class="o">=</span> <span class="n">SimpleBridge</span><span class="p">(</span><span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span>
                            <span class="n">optimizer</span>          <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_optimizer</span><span class="p">,</span>
                            <span class="n">learning_rate</span>      <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_learning_rate</span><span class="p">,</span>
                            <span class="n">gamma</span>              <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_gamma</span><span class="p">,</span>
                            <span class="n">initial_population</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_initial_population</span><span class="p">,</span>
                            <span class="n">batch_size</span>         <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_batch_size</span><span class="p">,</span>
                           <span class="p">)</span>
    
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_type</span> <span class="o">==</span> <span class="s2">&quot;r&quot;</span><span class="p">:</span> <span class="c1"># rainbow agent</span>
        <span class="n">agent</span> <span class="o">=</span> <span class="n">RainbowAgent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> 
                            <span class="n">devicestr</span>  <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_device</span><span class="p">,</span> 
                            <span class="n">gamma</span>              <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_gamma_exp</span><span class="p">,</span> 
                            <span class="n">buffer_size</span>        <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_buffer_size</span><span class="p">,</span>
                            <span class="n">target_net_sync</span>    <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_target_net_sync</span><span class="p">,</span>
                            <span class="n">steps_count</span>        <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_rain_steps_count</span><span class="p">,</span>
                            <span class="n">prio_replay_alpha</span>  <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">agent_rain_prio_replay_alpha</span>
                           <span class="p">)</span>

        <span class="n">bridge</span> <span class="o">=</span> <span class="n">RainbowBridge</span><span class="p">(</span><span class="n">agent</span><span class="o">=</span><span class="n">agent</span><span class="p">,</span>
                            <span class="n">optimizer</span>          <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_optimizer</span><span class="p">,</span>
                            <span class="n">learning_rate</span>      <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_learning_rate</span><span class="p">,</span>
                            <span class="n">gamma</span>              <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_gamma</span><span class="p">,</span>
                            <span class="n">initial_population</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_initial_population</span><span class="p">,</span>
                            <span class="n">batch_size</span>         <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_batch_size</span><span class="p">,</span>
                            <span class="n">beta_start</span>         <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_rain_beta_start</span><span class="p">,</span>
                            <span class="n">beta_frames</span>        <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">bridge_rain_beta_frames</span>
                           <span class="p">)</span>        
    
    <span class="n">control</span> <span class="o">=</span> <span class="n">LoopControl</span><span class="p">(</span>
                   <span class="n">bridge</span>              <span class="o">=</span> <span class="n">bridge</span><span class="p">,</span> 
                   <span class="n">run_name</span>            <span class="o">=</span> <span class="n">config_name</span><span class="p">,</span> 
                   <span class="n">bound_avg_reward</span>    <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">loop_bound_avg_reward</span><span class="p">,</span>
                   <span class="n">logtb</span>               <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">loop_logtb</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">control</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">run_example</span><span class="p">(</span><span class="n">config_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="c1"># get rid of missing metrics warning</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">UserWarning</span><span class="p">)</span>
    
    <span class="n">control</span> <span class="o">=</span> <span class="n">create_control</span><span class="p">(</span><span class="n">HYPERPARAMS</span><span class="p">[</span><span class="n">config_name</span><span class="p">],</span> <span class="n">config_name</span><span class="p">)</span>
    <span class="n">control</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># run_example(&#39;buffer_eps&#39;)</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cuda&#39;)</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cpu&#39;)</span>
<span class="c1">#run_example(&#39;buffer_eps_lr_cpu&#39;)</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cpu_limit&#39;) # abgebrochen ... hat immer hin und hergeschwankt</span>
<span class="c1">#run_example(&#39;r_buffer_eps&#39;) # abgebrochen.. hat ziemlich lange bei einer Episode gedreht</span>
<span class="c1">#run_example(&#39;r_buffer_eps_limit&#39;) # abgebrochen</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cpu_gamma&#39;) # abgebrochen ..</span>
<span class="c1"># run_example(&#39;buffer_eps_2_cpu_gamma_time&#39;) # abgebrochen.. nach ca. 40 minuten</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cpu_gamma_time_lr&#39;) #abgebrochen Achtung: Graf hat zeitlcken</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cpu_time_buffer&#39;) #buffersize doppelt, init auf 20&#39;000, Batchsize 48 -&gt; idee -&gt; ausgogener -&gt; abgebrochen: am Anfang gut uns stabiler</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cpu_time_buffer_lrbg&#39;)# abgebrochen </span>

<span class="c1"># ------------------ timestep im state</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cpu_gamma_time_state&#39;) # abgebrochen</span>
<span class="c1">#run_example(&#39;buffer_eps_2_cpu_timestate&#39;) # abgebrochen kommt nicht ans resultat von buffer_eps_2_cpu ran.. </span>

<span class="c1"># ------------------ NW Grssen angepasst 2 Layer a 4 Neuronen</span>
<span class="n">run_example</span><span class="p">(</span><span class="s1">&#39;buffer_eps_2_cpu_timestate_newHiddenL&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

