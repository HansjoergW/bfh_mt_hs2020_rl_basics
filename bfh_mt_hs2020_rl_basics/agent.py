# AUTOGENERATED! DO NOT EDIT! File to edit: 02_Agent.ipynb (unless otherwise specified).

__all__ = ['AgentBase', 'SimpleNet', 'SimpleAgent', 'NoisyFactorizedLinear', 'DuelingNet', 'RainbowAgent']

# Cell
from .env import CarEnv

from abc import ABC, abstractmethod
from typing import Iterable, Tuple, List

import torch

class AgentBase(ABC):

    def __init__(self, env: CarEnv, devicestr:str):
        self.env = env
        self.device = torch.device(devicestr)

    @abstractmethod
    def get_net(self):
        pass

    @abstractmethod
    def get_tgtnet(self):
        pass

    @abstractmethod
    def get_buffer(self):
        pass

    @abstractmethod
    def iteration_completed(self, iteration: int):
        pass


# Cell
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self, obs_size, hidden_size, n_actions):
        super(SimpleNet, self).__init__()

#         self.net = nn.Sequential(
#             nn.Linear(obs_size, hidden_size),
#             nn.ReLU(),
#             nn.Linear(hidden_size, n_actions)
#         )

        self.net = nn.Sequential(
            nn.Linear(obs_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, n_actions)
        )

    def forward(self, x):
        return self.net(x.float())

# Cell
from .env import CarEnv

import gym
import ptan
import torch
from torch import device

class SimpleAgent(AgentBase):

    def __init__(self, env: CarEnv,
                 devicestr:str,
                 gamma:float,
                 buffer_size:int,
                 target_net_sync:int = 1000,
                 eps_start:float = 1.0,
                 eps_final:float = 0.02,
                 eps_frames:int = 10**5):

        super(SimpleAgent, self).__init__(env, devicestr)

        self.target_net_sync = target_net_sync

        self.hiddensize = 128
        self.hiddensize = 4

        self.net = self._config_net()

        self.tgt_net = ptan.agent.TargetNet(self.net)

        self.selector = ptan.actions.EpsilonGreedyActionSelector(
                                    epsilon=1,
                                    selector=ptan.actions.ArgmaxActionSelector())

        self.epsilon_tracker = ptan.actions.EpsilonTracker(selector=self.selector, eps_start=eps_start, eps_final=eps_final, eps_frames=eps_frames)

        self.agent = agent = ptan.agent.DQNAgent(self.net, self.selector, device = self.device)

        self.exp_source = ptan.experience.ExperienceSourceFirstLast(self.env, self.agent, gamma=gamma)
        self.buffer = ptan.experience.ExperienceReplayBuffer(self.exp_source, buffer_size=buffer_size)


    def _config_net(self)-> nn.Module:
        return SimpleNet(self.env.observation_space.shape[0], self.hiddensize, self.env.action_space.n).to(self.device)


    def iteration_completed(self, iteration: int):

        self.epsilon_tracker.frame(iteration)

        if iteration % self.target_net_sync == 0:
            self.tgt_net.sync()

    def get_net(self):
        return self.net

    def get_tgtnet(self):
        return self.tgt_net

    def get_buffer(self):
        return self.buffer


# Cell
import torch.nn as nn
import math
from torch.nn import functional as F

class NoisyFactorizedLinear(nn.Linear):
    """
    NoisyNet layer with factorized gaussian noise

    N.B. nn.Linear already initializes weight and bias to
    """
    def __init__(self, in_features, out_features,
                 sigma_zero=0.4, bias=True):
        super(NoisyFactorizedLinear, self).__init__(
            in_features, out_features, bias=bias)
        sigma_init = sigma_zero / math.sqrt(in_features)
        w = torch.full((out_features, in_features), sigma_init)
        self.sigma_weight = nn.Parameter(w)
        z1 = torch.zeros(1, in_features)
        self.register_buffer("epsilon_input", z1)
        z2 = torch.zeros(out_features, 1)
        self.register_buffer("epsilon_output", z2)
        if bias:
            w = torch.full((out_features,), sigma_init)
            self.sigma_bias = nn.Parameter(w)

    def forward(self, input):
        self.epsilon_input.normal_()
        self.epsilon_output.normal_()

        func = lambda x: torch.sign(x) * \
                         torch.sqrt(torch.abs(x))
        eps_in = func(self.epsilon_input.data)
        eps_out = func(self.epsilon_output.data)

        bias = self.bias
        if bias is not None:
            bias = bias + self.sigma_bias * eps_out.t()
        noise_v = torch.mul(eps_in, eps_out)
        v = self.weight + self.sigma_weight * noise_v
        return F.linear(input, v, bias)


class DuelingNet(nn.Module):
    def __init__(self, obs_size, hidden_size, n_actions):
        super(DuelingNet, self).__init__()

        self.net_adv = nn.Sequential(
            NoisyFactorizedLinear(obs_size, hidden_size),
            nn.ReLU(),
            NoisyFactorizedLinear(hidden_size, n_actions)
        )

        self.net_val = nn.Sequential(
            NoisyFactorizedLinear(obs_size, hidden_size),
            nn.ReLU(),
            NoisyFactorizedLinear(hidden_size, 1)
        )

    def forward(self, x):
        val = self.net_val(x.float())
        adv = self.net_adv(x.float())

        return val + (adv - adv.mean(dim=1, keepdim=True))

# Cell
from .env import CarEnv

import gym
import ptan
import torch
from torch import device

class RainbowAgent(AgentBase):

    def __init__(self, env: CarEnv,
                 devicestr:str,
                 gamma:float,
                 buffer_size:int,
                 target_net_sync:int = 1000,
                 steps_count:int = 3,
                 prio_replay_alpha:float = 0.6):

        self.env = env
        self.steps_count = steps_count
        self.device = torch.device(devicestr)
        self.target_net_sync = target_net_sync

        self.net = self._config_net()

        self.tgt_net = ptan.agent.TargetNet(self.net)

        self.selector = ptan.actions.ArgmaxActionSelector()

        self.agent = agent = ptan.agent.DQNAgent(self.net, self.selector, device = self.device)

        self.exp_source = ptan.experience.ExperienceSourceFirstLast(self.env, self.agent, gamma=gamma, steps_count=self.steps_count)

        self.buffer = ptan.experience.PrioritizedReplayBuffer(self.exp_source, buffer_size=buffer_size, alpha=prio_replay_alpha)


    def _config_net(self)-> nn.Module:
        return DuelingNet(self.env.observation_space.shape[0], 128, self.env.action_space.n).to(self.device)


    def iteration_completed(self, iteration: int):

        if iteration % self.target_net_sync == 0:
            self.tgt_net.sync()


    def get_net(self):
        return self.net


    def get_tgtnet(self):
        return self.tgt_net


    def get_buffer(self):
        return self.buffer
