{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "# stellt sicher, dass beim verÃ¤ndern der core library diese wieder neu geladen wird\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bfh_mt_hs2020_rl_basics.env import CarEnv\n",
    "from bfh_mt_hs2020_rl_basics.agent import SimpleAgent, RainbowAgent\n",
    "from bfh_mt_hs2020_rl_basics.bridge import SimpleBridge, RainbowBridge\n",
    "from bfh_mt_hs2020_rl_basics.loop import LoopControl\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Tuple, Discrete, Box\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "from torch.optim import Optimizer, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMS = {\n",
    "    'base_setup': SimpleNamespace(**{\n",
    "            # env\n",
    "            'env_mode_energy_penalty'     : False,    # should there be a -1 point penalty for a used energy unit\n",
    "            'env_mode_random'             : False,    # does acceleration and decelartion have a random part\n",
    "            'env_mode_limit_steps'        : False,    # are the maximum possible steps limited\n",
    "            'env_mode_time_penalty'       : False,    # Penalty for every timestep\n",
    "        \n",
    "            # agent\n",
    "            'agent_type'                  : \"s\",      # agent type: s=simple, r=rainbow\n",
    "            'agent_device'                : \"cpu\",    # cpu or cuda\n",
    "            'agent_gamma_exp'             : 0.9,      # discount_factor for experience_first_last.. shouldn't matter since step_size is only 1\n",
    "            'agent_buffer_size'           : 1000,     # size of replay buffer\n",
    "            'agent_target_net_sync'       : 1000,     # sync TargetNet with weights of DNN every .. iterations\n",
    "            'agent_simple_eps_start'      : 1.0,      # simpleagent: epsilon start\n",
    "            'agent_simple_eps_final'      : 0.02,     # simpleagent: epsilon end\n",
    "            'agent_simple_eps_frames'     : 10**5,    # simpleagent: epsilon frames -> how many frames until 0.02 should be reached .. decay is linear\n",
    "            'agent_rain_steps_count'      : 1,        # rainbowagent: steps per iteration\n",
    "            'agent_rain_prio_replay_alpha': 0.6,      # rainbowagent: prio buffer alpha  \n",
    "        \n",
    "            # bridge  \n",
    "            'bridge_optimizer'            : None,     # Optimizer -> default ist Adam\n",
    "            'bridge_learning_rate'        : 0.0001,   # learningrate\n",
    "            'bridge_gamma'                : 0.9,      # discount_factor for reward\n",
    "            'bridge_initial_population'   : 1000,     # initial number of experiences in buffer\n",
    "            'bridge_batch_size'           : 32,       # batch_size for training\n",
    "            'bridge_rain_beta_start'      : 0.4,      # rainbow: start beta for Gewichtung buffer\n",
    "            'bridge_rain_beta_frames'     : 100000,   # rainbow: iteration when bea reaches 1\n",
    "\n",
    "            # loop control  \n",
    "            'loop_bound_avg_reward'       : 0.0,   # target avg reward\n",
    "            'loop_logtb'                  : True,     # Log to Tensorboard Logfile\n",
    "    }),\n",
    "  'buffer_eps': SimpleNamespace(**{\n",
    "            # env\n",
    "            'env_mode_energy_penalty'     : False,    # should there be a -1 point penalty for a used energy unit\n",
    "            'env_mode_random'             : False,    # does acceleration and decelartion have a random part\n",
    "            'env_mode_limit_steps'        : False,    # are the maximum possible steps limited\n",
    "            'env_mode_time_penalty'       : False,    # Penalty for every timestep\n",
    "      \n",
    "            # agent\n",
    "            'agent_type'                  : \"s\",      # agent type: s=simple, r=rainbow\n",
    "            'agent_device'                : \"cpu\",    # cpu or cuda\n",
    "            'agent_gamma_exp'             : 0.9,      # discount_factor for experience_first_last.. shouldn't matter since step_size is only 1\n",
    "            'agent_buffer_size'           : 50000,    # size of replay buffer\n",
    "            'agent_target_net_sync'       : 1000,     # sync TargetNet with weights of DNN every .. iterations\n",
    "            'agent_simple_eps_start'      : 1.0,      # simpleagent: epsilon start\n",
    "            'agent_simple_eps_final'      : 0.02,     # simpleagent: epsilon end\n",
    "            'agent_simple_eps_frames'     : 10**6,    # simpleagent: epsilon frames -> how many frames until 0.02 should be reached .. decay is linear\n",
    "            'agent_rain_steps_count'      : 1,        # rainbowagent: steps per iteration\n",
    "            'agent_rain_prio_replay_alpha': 0.6,      # rainbowagent: prio buffer alpha  \n",
    "  \n",
    "            # bridge  \n",
    "            'bridge_optimizer'            : None,     # Optimizer -> default ist Adam\n",
    "            'bridge_learning_rate'        : 0.0001,   # learningrate\n",
    "            'bridge_gamma'                : 0.9,      # discount_factor for reward\n",
    "            'bridge_initial_population'   : 5000,     # initial number of experiences in buffer\n",
    "            'bridge_batch_size'           : 32,       # batch_size for training\n",
    "            'bridge_rain_beta_start'      : 0.4,      # rainbow: start beta for Gewichtung buffer\n",
    "            'bridge_rain_beta_frames'     : 100000,   # rainbow: iteration when bea reaches 1\n",
    "\n",
    "            # loop control  \n",
    "            'loop_bound_avg_reward'       : 50.0,   # target avg reward\n",
    "            'loop_logtb'                  : True,     # Log to Tensorboard Logfile\n",
    "    }),\n",
    "  'buffer_eps_2_cuda': SimpleNamespace(**{\n",
    "            # env\n",
    "            'env_mode_energy_penalty'     : False,    # should there be a -1 point penalty for a used energy unit\n",
    "            'env_mode_random'             : False,    # does acceleration and decelartion have a random part\n",
    "            'env_mode_limit_steps'        : False,    # are the maximum possible steps limited\n",
    "            'env_mode_time_penalty'       : False,    # Penalty for every timestep\n",
    "\n",
    "            # agent\n",
    "            'agent_type'                  : \"s\",      # agent type: s=simple, r=rainbow\n",
    "            'agent_device'                : \"cuda\",   # * cpu or cuda\n",
    "            'agent_gamma_exp'             : 0.9,      # discount_factor for experience_first_last.. shouldn't matter since step_size is only 1\n",
    "            'agent_buffer_size'           : 50000,    # size of replay buffer\n",
    "            'agent_target_net_sync'       : 1000,     # sync TargetNet with weights of DNN every .. iterations\n",
    "            'agent_simple_eps_start'      : 1.0,      # simpleagent: epsilon start\n",
    "            'agent_simple_eps_final'      : 0.02,     # simpleagent: epsilon end\n",
    "            'agent_simple_eps_frames'     : 5*10**5,    # * simpleagent: epsilon frames -> how many frames until 0.02 should be reached .. decay is linear\n",
    "            'agent_rain_steps_count'      : 1,        # rainbowagent: steps per iteration\n",
    "            'agent_rain_prio_replay_alpha': 0.6,      # rainbowagent: prio buffer alpha  \n",
    "  \n",
    "            # bridge  \n",
    "            'bridge_optimizer'            : None,     # Optimizer -> default ist Adam\n",
    "            'bridge_learning_rate'        : 0.0001,   # learningrate\n",
    "            'bridge_gamma'                : 0.9,      # discount_factor for reward\n",
    "            'bridge_initial_population'   : 5000,     # initial number of experiences in buffer\n",
    "            'bridge_batch_size'           : 32,       # batch_size for training\n",
    "            'bridge_rain_beta_start'      : 0.4,      # rainbow: start beta for Gewichtung buffer\n",
    "            'bridge_rain_beta_frames'     : 100000,   # rainbow: iteration when bea reaches 1\n",
    "\n",
    "            # loop control  \n",
    "            'loop_bound_avg_reward'       : 50.0,   # target avg reward\n",
    "            'loop_logtb'                  : True,     # Log to Tensorboard Logfile\n",
    "    }),\n",
    "\n",
    "  'buffer_eps_2_cpu': SimpleNamespace(**{\n",
    "            # env\n",
    "            'env_mode_energy_penalty'     : False,    # should there be a -1 point penalty for a used energy unit\n",
    "            'env_mode_random'             : False,    # does acceleration and decelartion have a random part\n",
    "            'env_mode_limit_steps'        : False,    # are the maximum possible steps limited\n",
    "            'env_mode_time_penalty'       : False,    # Penalty for every timestep\n",
    "\n",
    "            # agent\n",
    "            'agent_type'                  : \"s\",      # agent type: s=simple, r=rainbow\n",
    "            'agent_device'                : \"cpu\",   # * cpu or cuda\n",
    "            'agent_gamma_exp'             : 0.9,      # discount_factor for experience_first_last.. shouldn't matter since step_size is only 1\n",
    "            'agent_buffer_size'           : 50000,    # size of replay buffer\n",
    "            'agent_target_net_sync'       : 1000,     # sync TargetNet with weights of DNN every .. iterations\n",
    "            'agent_simple_eps_start'      : 1.0,      # simpleagent: epsilon start\n",
    "            'agent_simple_eps_final'      : 0.02,     # simpleagent: epsilon end\n",
    "            'agent_simple_eps_frames'     : 5*10**5,    # * simpleagent: epsilon frames -> how many frames until 0.02 should be reached .. decay is linear\n",
    "            'agent_rain_steps_count'      : 1,        # rainbowagent: steps per iteration\n",
    "            'agent_rain_prio_replay_alpha': 0.6,      # rainbowagent: prio buffer alpha  \n",
    "  \n",
    "            # bridge  \n",
    "            'bridge_optimizer'            : None,     # Optimizer -> default ist Adam\n",
    "            'bridge_learning_rate'        : 0.0001,   # learningrate\n",
    "            'bridge_gamma'                : 0.9,      # discount_factor for reward\n",
    "            'bridge_initial_population'   : 5000,     # initial number of experiences in buffer\n",
    "            'bridge_batch_size'           : 32,       # batch_size for training\n",
    "            'bridge_rain_beta_start'      : 0.4,      # rainbow: start beta for Gewichtung buffer\n",
    "            'bridge_rain_beta_frames'     : 100000,   # rainbow: iteration when bea reaches 1\n",
    "    \n",
    "            # loop control  \n",
    "            'loop_bound_avg_reward'       : 50.0,     # target avg reward\n",
    "            'loop_logtb'                  : True,     # Log to Tensorboard Logfile\n",
    "    }),\n",
    "  'buffer_eps_lr_cpu': SimpleNamespace(**{\n",
    "            # env\n",
    "            'env_mode_energy_penalty'     : False,    # should there be a -1 point penalty for a used energy unit\n",
    "            'env_mode_random'             : False,    # does acceleration and decelartion have a random part\n",
    "            'env_mode_limit_steps'        : False,    # are the maximum possible steps limited\n",
    "            'env_mode_time_penalty'       : False,    # Penalty for every timestep\n",
    "      \n",
    "            # agent\n",
    "            'agent_type'                  : \"s\",      # agent type: s=simple, r=rainbow\n",
    "            'agent_device'                : \"cpu\",    # * cpu or cuda\n",
    "            'agent_gamma_exp'             : 0.9,      # discount_factor for experience_first_last.. shouldn't matter since step_size is only 1\n",
    "            'agent_buffer_size'           : 50000,    # size of replay buffer\n",
    "            'agent_target_net_sync'       : 1000,     # sync TargetNet with weights of DNN every .. iterations\n",
    "            'agent_simple_eps_start'      : 1.0,      # simpleagent: epsilon start\n",
    "            'agent_simple_eps_final'      : 0.02,     # simpleagent: epsilon end\n",
    "            'agent_simple_eps_frames'     : 5*10**5,  # * simpleagent: epsilon frames -> how many frames until 0.02 should be reached .. decay is linear\n",
    "            'agent_rain_steps_count'      : 1,        # rainbowagent: steps per iteration\n",
    "            'agent_rain_prio_replay_alpha': 0.6,      # rainbowagent: prio buffer alpha  \n",
    "  \n",
    "            # bridge  \n",
    "            'bridge_optimizer'            : None,     # Optimizer -> default ist Adam\n",
    "            'bridge_learning_rate'        : 0.00005,  # *learningrate\n",
    "            'bridge_gamma'                : 0.9,      # discount_factor for reward\n",
    "            'bridge_initial_population'   : 5000,     # initial number of experiences in buffer\n",
    "            'bridge_batch_size'           : 32,       # batch_size for training\n",
    "            'bridge_rain_beta_start'      : 0.4,      # rainbow: start beta for Gewichtung buffer\n",
    "            'bridge_rain_beta_frames'     : 100000,   # rainbow: iteration when bea reaches 1\n",
    "  \n",
    "            # loop control  \n",
    "            'loop_bound_avg_reward'       : 50.0,     # target avg reward\n",
    "            'loop_logtb'                  : True,     # Log to Tensorboard Logfile\n",
    "    }),\n",
    "  'buffer_eps_2_cpu_limit': SimpleNamespace(**{\n",
    "            # env\n",
    "            'env_mode_energy_penalty'     : False,    # should there be a -1 point penalty for a used energy unit\n",
    "            'env_mode_random'             : False,    # does acceleration and decelartion have a random part\n",
    "            'env_mode_limit_steps'        : True,     # *are the maximum possible steps limited\n",
    "            'env_mode_time_penalty'       : False,    # Penalty for every timestep\n",
    "\n",
    "            # agent\n",
    "            'agent_type'                  : \"s\",      # agent type: s=simple, r=rainbow\n",
    "            'agent_device'                : \"cpu\",   # * cpu or cuda\n",
    "            'agent_gamma_exp'             : 0.9,      # discount_factor for experience_first_last.. shouldn't matter since step_size is only 1\n",
    "            'agent_buffer_size'           : 50000,    # size of replay buffer\n",
    "            'agent_target_net_sync'       : 1000,     # sync TargetNet with weights of DNN every .. iterations\n",
    "            'agent_simple_eps_start'      : 1.0,      # simpleagent: epsilon start\n",
    "            'agent_simple_eps_final'      : 0.02,     # simpleagent: epsilon end\n",
    "            'agent_simple_eps_frames'     : 5*10**5,    # * simpleagent: epsilon frames -> how many frames until 0.02 should be reached .. decay is linear\n",
    "            'agent_rain_steps_count'      : 1,        # rainbowagent: steps per iteration\n",
    "            'agent_rain_prio_replay_alpha': 0.6,      # rainbowagent: prio buffer alpha  \n",
    "  \n",
    "            # bridge  \n",
    "            'bridge_optimizer'            : None,     # Optimizer -> default ist Adam\n",
    "            'bridge_learning_rate'        : 0.0001,   # learningrate\n",
    "            'bridge_gamma'                : 0.9,      # discount_factor for reward\n",
    "            'bridge_initial_population'   : 5000,     # initial number of experiences in buffer\n",
    "            'bridge_batch_size'           : 32,       # batch_size for training\n",
    "            'bridge_rain_beta_start'      : 0.4,      # rainbow: start beta for Gewichtung buffer\n",
    "            'bridge_rain_beta_frames'     : 100000,   # rainbow: iteration when bea reaches 1\n",
    "    \n",
    "            # loop control  \n",
    "            'loop_bound_avg_reward'       : 50.0,     # target avg reward\n",
    "            'loop_logtb'                  : True,     # Log to Tensorboard Logfile\n",
    "    }),\n",
    "  'r_buffer_eps': SimpleNamespace(**{\n",
    "            # env\n",
    "            'env_mode_energy_penalty'     : False,    # should there be a -1 point penalty for a used energy unit\n",
    "            'env_mode_random'             : False,    # does acceleration and decelartion have a random part\n",
    "            'env_mode_limit_steps'        : False,    # are the maximum possible steps limited\n",
    "            'env_mode_time_penalty'       : False,    # Penalty for every timestep\n",
    "      \n",
    "            # agent\n",
    "            'agent_type'                  : \"r\",      # *agent type: s=simple, r=rainbow\n",
    "            'agent_device'                : \"cpu\",    # cpu or cuda\n",
    "            'agent_gamma_exp'             : 0.9,      # discount_factor for experience_first_last.. shouldn't matter since step_size is only 1\n",
    "            'agent_buffer_size'           : 50000,    # size of replay buffer\n",
    "            'agent_target_net_sync'       : 1000,     # sync TargetNet with weights of DNN every .. iterations\n",
    "            'agent_simple_eps_start'      : 1.0,      # simpleagent: epsilon start\n",
    "            'agent_simple_eps_final'      : 0.02,     # simpleagent: epsilon end\n",
    "            'agent_simple_eps_frames'     : 5*10**5,  # *simpleagent: epsilon frames -> how many frames until 0.02 should be reached .. decay is linear\n",
    "            'agent_rain_steps_count'      : 3,        # rainbowagent: steps per iteration\n",
    "            'agent_rain_prio_replay_alpha': 0.6,      # rainbowagent: prio buffer alpha  \n",
    "  \n",
    "            # bridge  \n",
    "            'bridge_optimizer'            : None,     # Optimizer -> default ist Adam\n",
    "            'bridge_learning_rate'        : 0.0001,   # learningrate\n",
    "            'bridge_gamma'                : 0.9,      # discount_factor for reward\n",
    "            'bridge_initial_population'   : 5000,     # initial number of experiences in buffer\n",
    "            'bridge_batch_size'           : 32,       # batch_size for training\n",
    "            'bridge_rain_beta_start'      : 0.4,      # rainbow: start beta for Gewichtung buffer\n",
    "            'bridge_rain_beta_frames'     : 100000,   # rainbow: iteration when bea reaches 1\n",
    "\n",
    "            # loop control  \n",
    "            'loop_bound_avg_reward'       : 50.0,   # target avg reward\n",
    "            'loop_logtb'                  : True,     # Log to Tensorboard Logfile\n",
    "    })    ,\n",
    "  'r_buffer_eps_limit': SimpleNamespace(**{\n",
    "            # env\n",
    "            'env_mode_energy_penalty'     : False,    # should there be a -1 point penalty for a used energy unit\n",
    "            'env_mode_random'             : False,    # does acceleration and decelartion have a random part\n",
    "            'env_mode_limit_steps'        : True,    # are the maximum possible steps limited\n",
    "            'env_mode_time_penalty'       : False,    # Penalty for every timestep\n",
    "      \n",
    "            # agent\n",
    "            'agent_type'                  : \"r\",      # *agent type: s=simple, r=rainbow\n",
    "            'agent_device'                : \"cpu\",    # cpu or cuda\n",
    "            'agent_gamma_exp'             : 0.9,      # discount_factor for experience_first_last.. shouldn't matter since step_size is only 1\n",
    "            'agent_buffer_size'           : 50000,    # size of replay buffer\n",
    "            'agent_target_net_sync'       : 1000,     # sync TargetNet with weights of DNN every .. iterations\n",
    "            'agent_simple_eps_start'      : 1.0,      # simpleagent: epsilon start\n",
    "            'agent_simple_eps_final'      : 0.02,     # simpleagent: epsilon end\n",
    "            'agent_simple_eps_frames'     : 5*10**5,  # *simpleagent: epsilon frames -> how many frames until 0.02 should be reached .. decay is linear\n",
    "            'agent_rain_steps_count'      : 3,        # rainbowagent: steps per iteration\n",
    "            'agent_rain_prio_replay_alpha': 0.6,      # rainbowagent: prio buffer alpha  \n",
    "  \n",
    "            # bridge  \n",
    "            'bridge_optimizer'            : None,     # Optimizer -> default ist Adam\n",
    "            'bridge_learning_rate'        : 0.0001,   # learningrate\n",
    "            'bridge_gamma'                : 0.9,      # discount_factor for reward\n",
    "            'bridge_initial_population'   : 5000,     # initial number of experiences in buffer\n",
    "            'bridge_batch_size'           : 32,       # batch_size for training\n",
    "            'bridge_rain_beta_start'      : 0.4,      # rainbow: start beta for Gewichtung buffer\n",
    "            'bridge_rain_beta_frames'     : 100000,   # rainbow: iteration when bea reaches 1\n",
    "\n",
    "            # loop control  \n",
    "            'loop_bound_avg_reward'       : 50.0,     # target avg reward\n",
    "            'loop_logtb'                  : True,     # Log to Tensorboard Logfile\n",
    "    }),\n",
    "  'buffer_eps_2_cpu_gamma': SimpleNamespace(**{\n",
    "            # env\n",
    "            'env_mode_energy_penalty'     : False,    # should there be a -1 point penalty for a used energy unit\n",
    "            'env_mode_random'             : False,    # does acceleration and decelartion have a random part\n",
    "            'env_mode_limit_steps'        : False,    # are the maximum possible steps limited\n",
    "            'env_mode_time_penalty'       : False,    # Penalty for every timestep\n",
    "\n",
    "            # agent\n",
    "            'agent_type'                  : \"s\",      # agent type: s=simple, r=rainbow\n",
    "            'agent_device'                : \"cpu\",    # * cpu or cuda\n",
    "            'agent_gamma_exp'             : 0.9,      # discount_factor for experience_first_last.. shouldn't matter since step_size is only 1\n",
    "            'agent_buffer_size'           : 50000,    # size of replay buffer\n",
    "            'agent_target_net_sync'       : 1000,     # sync TargetNet with weights of DNN every .. iterations\n",
    "            'agent_simple_eps_start'      : 1.0,      # simpleagent: epsilon start\n",
    "            'agent_simple_eps_final'      : 0.02,     # simpleagent: epsilon end\n",
    "            'agent_simple_eps_frames'     : 5*10**5,  # * simpleagent: epsilon frames -> how many frames until 0.02 should be reached .. decay is linear\n",
    "            'agent_rain_steps_count'      : 1,        # rainbowagent: steps per iteration\n",
    "            'agent_rain_prio_replay_alpha': 0.6,      # rainbowagent: prio buffer alpha  \n",
    "  \n",
    "            # bridge  \n",
    "            'bridge_optimizer'            : None,     # Optimizer -> default ist Adam\n",
    "            'bridge_learning_rate'        : 0.0001,   # learningrate\n",
    "            'bridge_gamma'                : 0.99,     # *discount_factor for reward\n",
    "            'bridge_initial_population'   : 5000,     # initial number of experiences in buffer\n",
    "            'bridge_batch_size'           : 32,       # batch_size for training\n",
    "            'bridge_rain_beta_start'      : 0.4,      # rainbow: start beta for Gewichtung buffer\n",
    "            'bridge_rain_beta_frames'     : 100000,   # rainbow: iteration when bea reaches 1\n",
    "    \n",
    "            # loop control  \n",
    "            'loop_bound_avg_reward'       : 50.0,     # target avg reward\n",
    "            'loop_logtb'                  : True,     # Log to Tensorboard Logfile\n",
    "    }),\n",
    "  'buffer_eps_2_cpu_gamma_time': SimpleNamespace(**{\n",
    "            # env\n",
    "            'env_mode_energy_penalty'     : False,    # should there be a -1 point penalty for a used energy unit\n",
    "            'env_mode_random'             : False,    # does acceleration and decelartion have a random part\n",
    "            'env_mode_limit_steps'        : False,    # are the maximum possible steps limited\n",
    "            'env_mode_time_penalty'       : True,     # *Penalty for every timestep\n",
    "\n",
    "            # agent\n",
    "            'agent_type'                  : \"s\",      # agent type: s=simple, r=rainbow\n",
    "            'agent_device'                : \"cpu\",    # * cpu or cuda\n",
    "            'agent_gamma_exp'             : 0.9,      # discount_factor for experience_first_last.. shouldn't matter since step_size is only 1\n",
    "            'agent_buffer_size'           : 50000,    # size of replay buffer\n",
    "            'agent_target_net_sync'       : 1000,     # sync TargetNet with weights of DNN every .. iterations\n",
    "            'agent_simple_eps_start'      : 1.0,      # simpleagent: epsilon start\n",
    "            'agent_simple_eps_final'      : 0.02,     # simpleagent: epsilon end\n",
    "            'agent_simple_eps_frames'     : 5*10**5,  # * simpleagent: epsilon frames -> how many frames until 0.02 should be reached .. decay is linear\n",
    "            'agent_rain_steps_count'      : 1,        # rainbowagent: steps per iteration\n",
    "            'agent_rain_prio_replay_alpha': 0.6,      # rainbowagent: prio buffer alpha  \n",
    "  \n",
    "            # bridge  \n",
    "            'bridge_optimizer'            : None,     # Optimizer -> default ist Adam\n",
    "            'bridge_learning_rate'        : 0.0001,   # learningrate\n",
    "            'bridge_gamma'                : 0.99,     # *discount_factor for reward\n",
    "            'bridge_initial_population'   : 5000,     # initial number of experiences in buffer\n",
    "            'bridge_batch_size'           : 32,       # batch_size for training\n",
    "            'bridge_rain_beta_start'      : 0.4,      # rainbow: start beta for Gewichtung buffer\n",
    "            'bridge_rain_beta_frames'     : 100000,   # rainbow: iteration when bea reaches 1\n",
    "    \n",
    "            # loop control  \n",
    "            'loop_bound_avg_reward'       : 50.0,     # target avg reward\n",
    "            'loop_logtb'                  : True,     # Log to Tensorboard Logfile\n",
    "    }),\n",
    "  'buffer_eps_2_cpu_gamma_time_lr': SimpleNamespace(**{\n",
    "            # env\n",
    "            'env_mode_energy_penalty'     : False,    # should there be a -1 point penalty for a used energy unit\n",
    "            'env_mode_random'             : False,    # does acceleration and decelartion have a random part\n",
    "            'env_mode_limit_steps'        : False,    # are the maximum possible steps limited\n",
    "            'env_mode_time_penalty'       : True,     # *Penalty for every timestep\n",
    "\n",
    "            # agent\n",
    "            'agent_type'                  : \"s\",      # agent type: s=simple, r=rainbow\n",
    "            'agent_device'                : \"cpu\",    # * cpu or cuda\n",
    "            'agent_gamma_exp'             : 0.9,      # discount_factor for experience_first_last.. shouldn't matter since step_size is only 1\n",
    "            'agent_buffer_size'           : 50000,    # size of replay buffer\n",
    "            'agent_target_net_sync'       : 1000,     # sync TargetNet with weights of DNN every .. iterations\n",
    "            'agent_simple_eps_start'      : 1.0,      # simpleagent: epsilon start\n",
    "            'agent_simple_eps_final'      : 0.02,     # simpleagent: epsilon end\n",
    "            'agent_simple_eps_frames'     : 5*10**5,  # * simpleagent: epsilon frames -> how many frames until 0.02 should be reached .. decay is linear\n",
    "            'agent_rain_steps_count'      : 1,        # rainbowagent: steps per iteration\n",
    "            'agent_rain_prio_replay_alpha': 0.6,      # rainbowagent: prio buffer alpha  \n",
    "  \n",
    "            # bridge  \n",
    "            'bridge_optimizer'            : None,     # Optimizer -> default ist Adam\n",
    "            'bridge_learning_rate'        : 0.00005,  # *learningrate\n",
    "            'bridge_gamma'                : 0.99,     # *discount_factor for reward\n",
    "            'bridge_initial_population'   : 5000,     # initial number of experiences in buffer\n",
    "            'bridge_batch_size'           : 32,       # batch_size for training\n",
    "            'bridge_rain_beta_start'      : 0.4,      # rainbow: start beta for Gewichtung buffer\n",
    "            'bridge_rain_beta_frames'     : 100000,   # rainbow: iteration when bea reaches 1\n",
    "    \n",
    "            # loop control  \n",
    "            'loop_bound_avg_reward'       : -50.0,    # *target avg reward\n",
    "            'loop_logtb'                  : True,     # Log to Tensorboard Logfile\n",
    "    }),\n",
    "  'buffer_eps_2_cpu_time_buffer': SimpleNamespace(**{\n",
    "            # env\n",
    "            'env_mode_energy_penalty'     : False,    # should there be a -1 point penalty for a used energy unit\n",
    "            'env_mode_random'             : False,    # does acceleration and decelartion have a random part\n",
    "            'env_mode_limit_steps'        : False,    # are the maximum possible steps limited\n",
    "            'env_mode_time_penalty'       : True,     # *Penalty for every timestep\n",
    "\n",
    "            # agent\n",
    "            'agent_type'                  : \"s\",      # agent type: s=simple, r=rainbow\n",
    "            'agent_device'                : \"cpu\",    # * cpu or cuda\n",
    "            'agent_gamma_exp'             : 0.9,      # discount_factor for experience_first_last.. shouldn't matter since step_size is only 1\n",
    "            'agent_buffer_size'           : 100000,   # *size of replay buffer\n",
    "            'agent_target_net_sync'       : 1000,     # sync TargetNet with weights of DNN every .. iterations\n",
    "            'agent_simple_eps_start'      : 1.0,      # simpleagent: epsilon start\n",
    "            'agent_simple_eps_final'      : 0.02,     # simpleagent: epsilon end\n",
    "            'agent_simple_eps_frames'     : 5*10**5,  # * simpleagent: epsilon frames -> how many frames until 0.02 should be reached .. decay is linear\n",
    "            'agent_rain_steps_count'      : 1,        # rainbowagent: steps per iteration\n",
    "            'agent_rain_prio_replay_alpha': 0.6,      # rainbowagent: prio buffer alpha  \n",
    "  \n",
    "            # bridge  \n",
    "            'bridge_optimizer'            : None,     # Optimizer -> default ist Adam\n",
    "            'bridge_learning_rate'        : 0.0001,   # learningrate\n",
    "            'bridge_gamma'                : 0.99,     # *discount_factor for reward\n",
    "            'bridge_initial_population'   : 20000,    # *initial number of experiences in buffer\n",
    "            'bridge_batch_size'           : 48,       # *batch_size for training\n",
    "            'bridge_rain_beta_start'      : 0.4,      # rainbow: start beta for Gewichtung buffer\n",
    "            'bridge_rain_beta_frames'     : 100000,   # rainbow: iteration when bea reaches 1\n",
    "    \n",
    "            # loop control  \n",
    "            'loop_bound_avg_reward'       : -50.0,    # *target avg reward\n",
    "            'loop_logtb'                  : True,     # Log to Tensorboard Logfile\n",
    "    }), \n",
    "  'buffer_eps_2_cpu_time_buffer_lrbg': SimpleNamespace(**{\n",
    "            # env\n",
    "            'env_mode_energy_penalty'     : False,    # should there be a -1 point penalty for a used energy unit\n",
    "            'env_mode_random'             : False,    # does acceleration and decelartion have a random part\n",
    "            'env_mode_limit_steps'        : False,    # are the maximum possible steps limited\n",
    "            'env_mode_time_penalty'       : True,     # *Penalty for every timestep\n",
    "\n",
    "            # agent\n",
    "            'agent_type'                  : \"s\",      # agent type: s=simple, r=rainbow\n",
    "            'agent_device'                : \"cpu\",    # * cpu or cuda\n",
    "            'agent_gamma_exp'             : 0.9,      # discount_factor for experience_first_last.. shouldn't matter since step_size is only 1\n",
    "            'agent_buffer_size'           : 100000,   # *size of replay buffer\n",
    "            'agent_target_net_sync'       : 1000,     # sync TargetNet with weights of DNN every .. iterations\n",
    "            'agent_simple_eps_start'      : 1.0,      # simpleagent: epsilon start\n",
    "            'agent_simple_eps_final'      : 0.02,     # simpleagent: epsilon end\n",
    "            'agent_simple_eps_frames'     : 5*10**5,  # * simpleagent: epsilon frames -> how many frames until 0.02 should be reached .. decay is linear\n",
    "            'agent_rain_steps_count'      : 1,        # rainbowagent: steps per iteration\n",
    "            'agent_rain_prio_replay_alpha': 0.6,      # rainbowagent: prio buffer alpha  \n",
    "  \n",
    "            # bridge  \n",
    "            'bridge_optimizer'            : None,     # Optimizer -> default ist Adam\n",
    "            'bridge_learning_rate'        : 0.001,    # *learningrate\n",
    "            'bridge_gamma'                : 0.99,     # *discount_factor for reward\n",
    "            'bridge_initial_population'   : 20000,    # *initial number of experiences in buffer\n",
    "            'bridge_batch_size'           : 48,       # *batch_size for training\n",
    "            'bridge_rain_beta_start'      : 0.4,      # rainbow: start beta for Gewichtung buffer\n",
    "            'bridge_rain_beta_frames'     : 100000,   # rainbow: iteration when bea reaches 1\n",
    "    \n",
    "            # loop control  \n",
    "            'loop_bound_avg_reward'       : -50.0,    # *target avg reward\n",
    "            'loop_logtb'                  : True,     # Log to Tensorboard Logfile\n",
    "    }),     \n",
    "    \n",
    "    # ab hier mit timestep im state\n",
    "      'buffer_eps_2_cpu_gamma_time_state': SimpleNamespace(**{\n",
    "            # env\n",
    "            'env_mode_energy_penalty'     : False,    # should there be a -1 point penalty for a used energy unit\n",
    "            'env_mode_random'             : False,    # does acceleration and decelartion have a random part\n",
    "            'env_mode_limit_steps'        : False,    # are the maximum possible steps limited\n",
    "            'env_mode_time_penalty'       : True,     # *Penalty for every timestep\n",
    "\n",
    "            # agent\n",
    "            'agent_type'                  : \"s\",      # agent type: s=simple, r=rainbow\n",
    "            'agent_device'                : \"cpu\",    # * cpu or cuda\n",
    "            'agent_gamma_exp'             : 0.9,      # discount_factor for experience_first_last.. shouldn't matter since step_size is only 1\n",
    "            'agent_buffer_size'           : 50000,    # size of replay buffer\n",
    "            'agent_target_net_sync'       : 1000,     # sync TargetNet with weights of DNN every .. iterations\n",
    "            'agent_simple_eps_start'      : 1.0,      # simpleagent: epsilon start\n",
    "            'agent_simple_eps_final'      : 0.02,     # simpleagent: epsilon end\n",
    "            'agent_simple_eps_frames'     : 5*10**5,  # * simpleagent: epsilon frames -> how many frames until 0.02 should be reached .. decay is linear\n",
    "            'agent_rain_steps_count'      : 1,        # rainbowagent: steps per iteration\n",
    "            'agent_rain_prio_replay_alpha': 0.6,      # rainbowagent: prio buffer alpha  \n",
    "  \n",
    "            # bridge  \n",
    "            'bridge_optimizer'            : None,     # Optimizer -> default ist Adam\n",
    "            'bridge_learning_rate'        : 0.0001,   # learningrate\n",
    "            'bridge_gamma'                : 0.99,     # *discount_factor for reward\n",
    "            'bridge_initial_population'   : 5000,     # initial number of experiences in buffer\n",
    "            'bridge_batch_size'           : 32,       # batch_size for training\n",
    "            'bridge_rain_beta_start'      : 0.4,      # rainbow: start beta for Gewichtung buffer\n",
    "            'bridge_rain_beta_frames'     : 100000,   # rainbow: iteration when bea reaches 1\n",
    "    \n",
    "            # loop control  \n",
    "            'loop_bound_avg_reward'       : 50.0,     # target avg reward\n",
    "            'loop_logtb'                  : True,     # Log to Tensorboard Logfile\n",
    "    }),\n",
    "    'buffer_eps_2_cpu_timestate': SimpleNamespace(**{\n",
    "            # env\n",
    "            'env_mode_energy_penalty'     : False,    # should there be a -1 point penalty for a used energy unit\n",
    "            'env_mode_random'             : False,    # does acceleration and decelartion have a random part\n",
    "            'env_mode_limit_steps'        : False,    # are the maximum possible steps limited\n",
    "            'env_mode_time_penalty'       : False,    # Penalty for every timestep\n",
    "\n",
    "            # agent\n",
    "            'agent_type'                  : \"s\",      # agent type: s=simple, r=rainbow\n",
    "            'agent_device'                : \"cpu\",   # * cpu or cuda\n",
    "            'agent_gamma_exp'             : 0.9,      # discount_factor for experience_first_last.. shouldn't matter since step_size is only 1\n",
    "            'agent_buffer_size'           : 50000,    # size of replay buffer\n",
    "            'agent_target_net_sync'       : 1000,     # sync TargetNet with weights of DNN every .. iterations\n",
    "            'agent_simple_eps_start'      : 1.0,      # simpleagent: epsilon start\n",
    "            'agent_simple_eps_final'      : 0.02,     # simpleagent: epsilon end\n",
    "            'agent_simple_eps_frames'     : 5*10**5,    # * simpleagent: epsilon frames -> how many frames until 0.02 should be reached .. decay is linear\n",
    "            'agent_rain_steps_count'      : 1,        # rainbowagent: steps per iteration\n",
    "            'agent_rain_prio_replay_alpha': 0.6,      # rainbowagent: prio buffer alpha  \n",
    "  \n",
    "            # bridge  \n",
    "            'bridge_optimizer'            : None,     # Optimizer -> default ist Adam\n",
    "            'bridge_learning_rate'        : 0.0001,   # learningrate\n",
    "            'bridge_gamma'                : 0.9,      # discount_factor for reward\n",
    "            'bridge_initial_population'   : 5000,     # initial number of experiences in buffer\n",
    "            'bridge_batch_size'           : 32,       # batch_size for training\n",
    "            'bridge_rain_beta_start'      : 0.4,      # rainbow: start beta for Gewichtung buffer\n",
    "            'bridge_rain_beta_frames'     : 100000,   # rainbow: iteration when bea reaches 1\n",
    "    \n",
    "            # loop control  \n",
    "            'loop_bound_avg_reward'       : 50.0,     # target avg reward\n",
    "            'loop_logtb'                  : True,     # Log to Tensorboard Logfile\n",
    "    }),\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_control(params: SimpleNamespace, config_name) -> LoopControl:\n",
    "    \n",
    "    env = CarEnv(mode_energy_penalty   = params.env_mode_energy_penalty, \n",
    "                 mode_random           = params.env_mode_random, \n",
    "                 mode_limit_steps      = params.env_mode_limit_steps,\n",
    "                 mode_time_penalty     = params.env_mode_time_penalty)\n",
    "    \n",
    "    if params.agent_type == \"s\": # simple agent\n",
    "        agent = SimpleAgent(env, \n",
    "                            devicestr  = params.agent_device, \n",
    "                            gamma           = params.agent_gamma_exp, \n",
    "                            buffer_size     = params.agent_buffer_size,\n",
    "                            target_net_sync = params.agent_target_net_sync,\n",
    "                            eps_start       = params.agent_simple_eps_start,\n",
    "                            eps_final       = params.agent_simple_eps_final,\n",
    "                            eps_frames      = params.agent_simple_eps_frames,\n",
    "                           )\n",
    "\n",
    "        bridge = SimpleBridge(agent=agent,\n",
    "                            optimizer          = params.bridge_optimizer,\n",
    "                            learning_rate      = params.bridge_learning_rate,\n",
    "                            gamma              = params.bridge_gamma,\n",
    "                            initial_population = params.bridge_initial_population,\n",
    "                            batch_size         = params.bridge_batch_size,\n",
    "                           )\n",
    "    \n",
    "    if params.agent_type == \"r\": # rainbow agent\n",
    "        agent = RainbowAgent(env, \n",
    "                            devicestr  = params.agent_device, \n",
    "                            gamma              = params.agent_gamma_exp, \n",
    "                            buffer_size        = params.agent_buffer_size,\n",
    "                            target_net_sync    = params.agent_target_net_sync,\n",
    "                            steps_count        = params.agent_rain_steps_count,\n",
    "                            prio_replay_alpha  = params.agent_rain_prio_replay_alpha\n",
    "                           )\n",
    "\n",
    "        bridge = RainbowBridge(agent=agent,\n",
    "                            optimizer          = params.bridge_optimizer,\n",
    "                            learning_rate      = params.bridge_learning_rate,\n",
    "                            gamma              = params.bridge_gamma,\n",
    "                            initial_population = params.bridge_initial_population,\n",
    "                            batch_size         = params.bridge_batch_size,\n",
    "                            beta_start         = params.bridge_rain_beta_start,\n",
    "                            beta_frames        = params.bridge_rain_beta_frames\n",
    "                           )        \n",
    "    \n",
    "    control = LoopControl(\n",
    "                   bridge              = bridge, \n",
    "                   run_name            = config_name, \n",
    "                   bound_avg_reward    = params.loop_bound_avg_reward,\n",
    "                   logtb               = params.loop_logtb)\n",
    "    \n",
    "    return control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(config_name: str):\n",
    "    # get rid of missing metrics warning\n",
    "    warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "    \n",
    "    control = create_control(HYPERPARAMS[config_name], config_name)\n",
    "    control.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_example('base_setup')\n",
    "# run_example('buffer_eps')\n",
    "#run_example('buffer_eps_2_cuda')\n",
    "#run_example('buffer_eps_2_cpu')\n",
    "#run_example('buffer_eps_lr_cpu')\n",
    "#run_example('buffer_eps_2_cpu_limit') # abgebrochen ... hat immer hin und hergeschwankt\n",
    "#run_example('r_buffer_eps') # abgebrochen.. hat ziemlich lange bei einer Episode gedreht\n",
    "#run_example('r_buffer_eps_limit') # abgebrochen\n",
    "#run_example('buffer_eps_2_cpu_gamma') # abgebrochen ..\n",
    "# run_example('buffer_eps_2_cpu_gamma_time') # abgebrochen.. nach ca. 40 minuten\n",
    "#run_example('buffer_eps_2_cpu_gamma_time_lr') #abgebrochen Achtung: Graf hat zeitlÃ¼cken\n",
    "#run_example('buffer_eps_2_cpu_time_buffer') #buffersize doppelt, init auf 20'000, Batchsize 48 -> idee -> ausgogener -> abgebrochen: am Anfang gut uns stabiler\n",
    "#run_example('buffer_eps_2_cpu_time_buffer_lrbg')# abgebrochen \n",
    "\n",
    "# ------------------ timestep im state\n",
    "#run_example('buffer_eps_2_cpu_gamma_time_state') # abgebrochen\n",
    "run_example('buffer_eps_2_cpu_timestate') # abgebrochen kommt nicht ans resultat von buffer_eps_2_cpu ran.. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
